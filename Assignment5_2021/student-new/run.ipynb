{"cells":[{"cell_type":"markdown","metadata":{"id":"Hyi_CBb1ttnm"},"source":["### Mount Google Drive and Check the possiblity to GPU"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17859,"status":"ok","timestamp":1636003276100,"user":{"displayName":"오승현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08073038130517070928"},"user_tz":-540},"id":"Awsa6PsOL44O","outputId":"590ec6cd-b56b-412d-eb6a-a3e7ee04c01c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":270,"status":"ok","timestamp":1636003283240,"user":{"displayName":"오승현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08073038130517070928"},"user_tz":-540},"id":"1YMBsSQDZe_g","outputId":"7d8453cc-6068-43af-b619-81b1a4df2d8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Nov  4 05:21:22 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"2eqjXUx2z_38"},"source":["### Run the Code with Non-pretraining model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":166954,"status":"ok","timestamp":1635400403894,"user":{"displayName":"오승현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08073038130517070928"},"user_tz":-540},"id":"xydCIpFdMYir","outputId":"4d61fee8-8663-42ad-f31e-3f19d4afe033"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello World\n","wiki.txt\n","vanilla.model.params\n","birth_places_train.tsv\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","epoch 1 iter 7: train loss 3.56898. lr 5.999844e-04: 100% 8/8 [00:05<00:00,  1.40it/s]\n","epoch 2 iter 7: train loss 2.73992. lr 5.999351e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 3 iter 7: train loss 2.39064. lr 5.998521e-04: 100% 8/8 [00:05<00:00,  1.52it/s]\n","epoch 4 iter 7: train loss 2.18724. lr 5.997352e-04: 100% 8/8 [00:05<00:00,  1.52it/s]\n","epoch 5 iter 7: train loss 2.08688. lr 5.995847e-04: 100% 8/8 [00:05<00:00,  1.51it/s]\n","epoch 6 iter 7: train loss 2.02493. lr 5.994004e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 7 iter 7: train loss 1.94851. lr 5.991823e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 8 iter 7: train loss 1.90396. lr 5.989306e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 9 iter 7: train loss 1.85652. lr 5.986453e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 10 iter 7: train loss 1.82134. lr 5.983263e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 11 iter 7: train loss 1.77353. lr 5.979737e-04: 100% 8/8 [00:05<00:00,  1.47it/s]\n","epoch 12 iter 7: train loss 1.69755. lr 5.975876e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 13 iter 7: train loss 1.65394. lr 5.971680e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 14 iter 7: train loss 1.57487. lr 5.967149e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 15 iter 7: train loss 1.51228. lr 5.962284e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 16 iter 7: train loss 1.43629. lr 5.957086e-04: 100% 8/8 [00:05<00:00,  1.47it/s]\n","epoch 17 iter 7: train loss 1.33797. lr 5.951554e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 18 iter 7: train loss 1.27068. lr 5.945690e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 19 iter 7: train loss 1.22218. lr 5.939495e-04: 100% 8/8 [00:05<00:00,  1.47it/s]\n","epoch 20 iter 7: train loss 1.17732. lr 5.932969e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 21 iter 7: train loss 1.07803. lr 5.926112e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 22 iter 7: train loss 1.06502. lr 5.918926e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 23 iter 7: train loss 1.01999. lr 5.911412e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 24 iter 7: train loss 0.98056. lr 5.903569e-04: 100% 8/8 [00:05<00:00,  1.47it/s]\n","epoch 25 iter 7: train loss 0.93424. lr 5.895400e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 26 iter 7: train loss 0.87279. lr 5.886905e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 27 iter 7: train loss 0.86547. lr 5.878084e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 28 iter 7: train loss 0.82457. lr 5.868940e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 29 iter 7: train loss 0.82977. lr 5.859473e-04: 100% 8/8 [00:05<00:00,  1.47it/s]\n","epoch 30 iter 7: train loss 0.79743. lr 5.849683e-04: 100% 8/8 [00:05<00:00,  1.47it/s]\n","epoch 31 iter 7: train loss 0.77523. lr 5.839573e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 32 iter 7: train loss 0.75288. lr 5.829143e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 33 iter 7: train loss 0.71036. lr 5.818395e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 34 iter 7: train loss 0.69113. lr 5.807329e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 35 iter 7: train loss 0.68783. lr 5.795947e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 36 iter 7: train loss 0.63211. lr 5.784251e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 37 iter 7: train loss 0.64610. lr 5.772241e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 38 iter 7: train loss 0.63729. lr 5.759918e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 39 iter 7: train loss 0.59693. lr 5.747285e-04: 100% 8/8 [00:05<00:00,  1.47it/s]\n","epoch 40 iter 7: train loss 0.59072. lr 5.734343e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 41 iter 7: train loss 0.60369. lr 5.721093e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 42 iter 7: train loss 0.57337. lr 5.707537e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 43 iter 7: train loss 0.58076. lr 5.693675e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 44 iter 7: train loss 0.52527. lr 5.679511e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 45 iter 7: train loss 0.54822. lr 5.665044e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 46 iter 7: train loss 0.51768. lr 5.650278e-04: 100% 8/8 [00:05<00:00,  1.51it/s]\n","epoch 47 iter 7: train loss 0.50756. lr 5.635213e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 48 iter 7: train loss 0.49335. lr 5.619852e-04: 100% 8/8 [00:05<00:00,  1.47it/s]\n","epoch 49 iter 7: train loss 0.47496. lr 5.604195e-04: 100% 8/8 [00:05<00:00,  1.47it/s]\n","epoch 50 iter 7: train loss 0.48072. lr 5.588246e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 51 iter 7: train loss 0.46955. lr 5.572005e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 52 iter 7: train loss 0.45615. lr 5.555474e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 53 iter 7: train loss 0.44991. lr 5.538656e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 54 iter 7: train loss 0.42411. lr 5.521552e-04: 100% 8/8 [00:05<00:00,  1.51it/s]\n","epoch 55 iter 7: train loss 0.42224. lr 5.504164e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 56 iter 7: train loss 0.39117. lr 5.486494e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 57 iter 7: train loss 0.41183. lr 5.468544e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 58 iter 7: train loss 0.38239. lr 5.450316e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 59 iter 7: train loss 0.37153. lr 5.431812e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 60 iter 7: train loss 0.32524. lr 5.413034e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 61 iter 7: train loss 0.33538. lr 5.393985e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 62 iter 7: train loss 0.32668. lr 5.374666e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 63 iter 7: train loss 0.32762. lr 5.355080e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 64 iter 7: train loss 0.31675. lr 5.335229e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 65 iter 7: train loss 0.31336. lr 5.315115e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 66 iter 7: train loss 0.28960. lr 5.294740e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 67 iter 7: train loss 0.29387. lr 5.274107e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 68 iter 7: train loss 0.26245. lr 5.253217e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 69 iter 7: train loss 0.24031. lr 5.232074e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 70 iter 7: train loss 0.25063. lr 5.210680e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 71 iter 7: train loss 0.21711. lr 5.189037e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 72 iter 7: train loss 0.23322. lr 5.167147e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 73 iter 7: train loss 0.22289. lr 5.145014e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 74 iter 7: train loss 0.20377. lr 5.122639e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 75 iter 7: train loss 0.19352. lr 5.100024e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 1 iter 7: train loss 0.36017. lr 5.999844e-04: 100% 8/8 [00:05<00:00,  1.51it/s]\n","epoch 2 iter 7: train loss 0.29760. lr 5.999351e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 3 iter 7: train loss 0.24712. lr 5.998521e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 4 iter 7: train loss 0.22264. lr 5.997352e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 5 iter 7: train loss 0.19593. lr 5.995847e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 6 iter 7: train loss 0.18209. lr 5.994004e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 7 iter 7: train loss 0.20867. lr 5.991823e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 8 iter 7: train loss 0.14543. lr 5.989306e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 9 iter 7: train loss 0.16609. lr 5.986453e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 10 iter 7: train loss 0.15478. lr 5.983263e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 11 iter 7: train loss 0.15386. lr 5.979737e-04: 100% 8/8 [00:05<00:00,  1.47it/s]\n","epoch 12 iter 7: train loss 0.13014. lr 5.975876e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 13 iter 7: train loss 0.11348. lr 5.971680e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 14 iter 7: train loss 0.15281. lr 5.967149e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 15 iter 7: train loss 0.13057. lr 5.962284e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 16 iter 7: train loss 0.12651. lr 5.957086e-04: 100% 8/8 [00:05<00:00,  1.47it/s]\n","epoch 17 iter 7: train loss 0.15192. lr 5.951554e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 18 iter 7: train loss 0.13205. lr 5.945690e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 19 iter 7: train loss 0.10771. lr 5.939495e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 20 iter 7: train loss 0.14664. lr 5.932969e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 21 iter 7: train loss 0.11670. lr 5.926112e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 22 iter 7: train loss 0.09652. lr 5.918926e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 23 iter 7: train loss 0.08301. lr 5.911412e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 24 iter 7: train loss 0.09074. lr 5.903569e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 25 iter 7: train loss 0.09347. lr 5.895400e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 26 iter 7: train loss 0.10042. lr 5.886905e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 27 iter 7: train loss 0.09396. lr 5.878084e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 28 iter 7: train loss 0.08665. lr 5.868940e-04: 100% 8/8 [00:05<00:00,  1.51it/s]\n","epoch 29 iter 7: train loss 0.11197. lr 5.859473e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 30 iter 7: train loss 0.07989. lr 5.849683e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 31 iter 7: train loss 0.08335. lr 5.839573e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 32 iter 7: train loss 0.10721. lr 5.829143e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 33 iter 7: train loss 0.08712. lr 5.818395e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 34 iter 7: train loss 0.08388. lr 5.807329e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 35 iter 7: train loss 0.07328. lr 5.795947e-04: 100% 8/8 [00:05<00:00,  1.51it/s]\n","epoch 36 iter 7: train loss 0.07821. lr 5.784251e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 37 iter 7: train loss 0.07898. lr 5.772241e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 38 iter 7: train loss 0.06841. lr 5.759918e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 39 iter 7: train loss 0.09147. lr 5.747285e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 40 iter 7: train loss 0.07306. lr 5.734343e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 41 iter 7: train loss 0.07458. lr 5.721093e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 42 iter 7: train loss 0.06774. lr 5.707537e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 43 iter 7: train loss 0.05779. lr 5.693675e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 44 iter 7: train loss 0.08363. lr 5.679511e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 45 iter 7: train loss 0.06074. lr 5.665044e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 46 iter 7: train loss 0.07511. lr 5.650278e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 47 iter 7: train loss 0.08650. lr 5.635213e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 48 iter 7: train loss 0.06649. lr 5.619852e-04: 100% 8/8 [00:05<00:00,  1.47it/s]\n","epoch 49 iter 7: train loss 0.07229. lr 5.604195e-04: 100% 8/8 [00:05<00:00,  1.51it/s]\n","epoch 50 iter 7: train loss 0.06935. lr 5.588246e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 51 iter 7: train loss 0.06902. lr 5.572005e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 52 iter 7: train loss 0.05737. lr 5.555474e-04: 100% 8/8 [00:05<00:00,  1.50it/s]\n","epoch 53 iter 7: train loss 0.05447. lr 5.538656e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 54 iter 7: train loss 0.05089. lr 5.521552e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 55 iter 7: train loss 0.05084. lr 5.504164e-04: 100% 8/8 [00:05<00:00,  1.51it/s]\n","epoch 56 iter 7: train loss 0.06186. lr 5.486494e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 57 iter 7: train loss 0.06204. lr 5.468544e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 58 iter 7: train loss 0.05038. lr 5.450316e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 59 iter 7: train loss 0.07244. lr 5.431812e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 60 iter 7: train loss 0.07454. lr 5.413034e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 61 iter 7: train loss 0.05151. lr 5.393985e-04: 100% 8/8 [00:05<00:00,  1.47it/s]\n","epoch 62 iter 7: train loss 0.06126. lr 5.374666e-04: 100% 8/8 [00:05<00:00,  1.47it/s]\n","epoch 63 iter 7: train loss 0.04556. lr 5.355080e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 64 iter 7: train loss 0.05775. lr 5.335229e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 65 iter 7: train loss 0.04606. lr 5.315115e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 66 iter 7: train loss 0.04557. lr 5.294740e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 67 iter 7: train loss 0.05013. lr 5.274107e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 68 iter 7: train loss 0.05615. lr 5.253217e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 69 iter 7: train loss 0.05167. lr 5.232074e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 70 iter 7: train loss 0.05223. lr 5.210680e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 71 iter 7: train loss 0.04213. lr 5.189037e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 72 iter 7: train loss 0.05809. lr 5.167147e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 73 iter 7: train loss 0.04461. lr 5.145014e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n","epoch 74 iter 7: train loss 0.05003. lr 5.122639e-04: 100% 8/8 [00:05<00:00,  1.48it/s]\n","epoch 75 iter 7: train loss 0.03738. lr 5.100024e-04: 100% 8/8 [00:05<00:00,  1.49it/s]\n"]}],"source":["!python /content/drive/MyDrive/speech_research/lecture/a5_2020/student-new/src/run.py finetune vanilla wiki.txt --writing_params_path vanilla.model.params --finetune_corpus_path birth_places_train.tsv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75611,"status":"ok","timestamp":1635402981271,"user":{"displayName":"오승현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08073038130517070928"},"user_tz":-540},"id":"yySFES60eW8H","outputId":"6b020d16-299d-4b80-9300-93f75b10d27a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello World\n","wiki.txt\n","vanilla.model.params\n","birth_dev.tsv\n","vanilla.nopretrain.dev.predictions\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","500it [01:11,  6.97it/s]\n","Correct: 1.0 out of 500.0: 0.2%\n"]}],"source":["!python /content/drive/MyDrive/speech_research/lecture/a5_2020/student-new/src/run.py evaluate vanilla wiki.txt --reading_params_path vanilla.model.params --eval_corpus_path birth_dev.tsv --outputs_path vanilla.nopretrain.dev.predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":62640,"status":"ok","timestamp":1635404314922,"user":{"displayName":"오승현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08073038130517070928"},"user_tz":-540},"id":"K-BkYIBwj0xZ","outputId":"4172ac72-2967-4ad4-f160-a204c1b4aeba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello World\n","wiki.txt\n","vanilla.model.params\n","birth_test_inputs.tsv\n","vanilla.nopretrain.test.predictions\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","/content/drive/MyDrive/speech_research/lecture/Assignment5_2021/student-new/vanilla.nopretrain.test.predictions /content/drive/MyDrive/speech_research/lecture/Assignment5_2021/student-new/birth_test_inputs.tsv\n","437it [00:58,  7.44it/s]\n","No gold birth places provided; returning (0,0)\n","Predictions written to /content/drive/MyDrive/speech_research/lecture/Assignment5_2021/student-new/vanilla.nopretrain.test.predictions; no targets provided\n"]}],"source":["!python /content/drive/MyDrive/speech_research/lecture/a5_2020/student-new/src/run.py evaluate vanilla wiki.txt --reading_params_path vanilla.model.params --eval_corpus_path birth_test_inputs.tsv --outputs_path vanilla.nopretrain.test.predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1474,"status":"ok","timestamp":1635405019039,"user":{"displayName":"오승현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08073038130517070928"},"user_tz":-540},"id":"QnlOVymZk3dj","outputId":"5f8f9fbc-b1ba-4f22-b545-f0c5281a501d"},"outputs":[{"name":"stdout","output_type":"stream","text":["birth_dev.tsv\n","[['Where was Bryan Dubreuiel born?', 'Atlanta'], ['Where was Ralf Wadephul born?', 'Berlin'], ['Where was Joseph Baggaley born?', 'England'], ['Where was Sandhya Sanjana born?', 'Mumbai'], ['Where was Alfred Mele born?', 'Detroit'], ['Where was Murray Esler born?', 'Geelong'], ['Where was Ryan Barton born?', 'Australia'], ['Where was Ernst Florian Winter born?', 'Vienna'], ['Where was Salvatore Papaccio born?', 'Naples'], ['Where was Raymond Brown born?', 'Ontario'], ['Where was Bruno Rubess born?', 'Riga'], ['Where was Vivek Razdan born?', 'Delhi'], ['Where was Pete Winslow born?', 'Washington'], ['Where was Trevor Taylor born?', 'Sheffield'], ['Where was Richard Eddy born?', 'Providence'], ['Where was Michele Radosevich born?', 'Minneapolis'], ['Where was Catherine Keller born?', 'Paris'], ['Where was Richard Cobbold born?', 'Ipswich'], ['Where was Zhang Xueling born?', 'Beijing'], ['Where was William Mackenzie McLeod born?', 'Sydney'], ['Where was Erna Paris born?', 'Toronto'], ['Where was Edward Searing born?', 'Aurora'], ['Where was Theodore Pollock Ferguson born?', 'Mansfield'], ['Where was Pietro Delfino born?', 'Venice'], ['Where was George Alexander Pearre born?', 'Cumberland'], ['Where was Alessandro Vollero born?', 'Italy'], ['Where was Diane Greene born?', 'Rochester'], ['Where was Sy Landy born?', 'Brooklyn'], ['Where was Michio Ihara born?', 'Paris'], ['Where was Rafael Nieto Abeill%C3%A9 born?', 'Cuba'], ['Where was Eric Goldberg born?', 'Berlin'], ['Where was Baron Wormser born?', 'Baltimore'], ['Where was Asli Sungu born?', 'Istanbul'], ['Where was Eija Krogerus born?', 'Helsinki'], ['Where was William Hare born?', 'Leicester'], ['Where was Gaurav Keerthi born?', 'India'], ['Where was John Thorne born?', 'Quincy'], ['Where was Konrad Wasiela born?', 'Vancouver'], ['Where was Jeremy Holmes born?', 'London'], ['Where was Daniel M. Hausman born?', 'Chicago'], ['Where was Peter Brown born?', 'Ireland'], ['Where was Janne Parviainen born?', 'Finland'], ['Where was Eric Boniface born?', 'France'], ['Where was C%C3%A9sar de Oliveira born?', 'Porto'], ['Where was Leo Treitler born?', 'Dortmund'], ['Where was Moses Mason, Jr. born?', 'Dublin'], ['Where was Denis McQuade born?', 'Glasgow'], ['Where was Nerses Yeritsyan born?', 'Yerevan'], ['Where was John Henry born?', 'Dublin'], ['Where was Adrian Baker born?', 'London'], ['Where was Fritz Noll born?', 'Frankfurt'], ['Where was Adam Kowalczyk born?', 'York'], ['Where was Megan Spencer born?', 'Australia'], ['Where was George William Houghton born?', 'Perth'], ['Where was Marcus Dods born?', 'Edinburgh'], ['Where was Nicola Campogrande born?', 'Turin'], ['Where was Michael Falcon born?', 'Norwich'], ['Where was Margaret Haile born?', 'Canada'], ['Where was John Percival Postgate born?', 'Birmingham'], ['Where was Walter Lupi born?', 'Milan'], ['Where was George Hurst born?', 'Edinburgh'], ['Where was Albert McElroy born?', 'Glasgow'], ['Where was Witold Rodziński born?', 'Lviv'], ['Where was Mika Salmi born?', 'Helsinki'], ['Where was Martin Grams, Jr. born?', 'Baltimore'], ['Where was Arthur Nichols born?', 'Bristol'], ['Where was Giulio Carpioni born?', 'Venice'], ['Where was Bhupinder Nath Kirpal born?', 'Lahore'], ['Where was Francisco Ortiz de Vergara born?', 'Seville'], ['Where was Ahmed Djoghlaf born?', 'Algiers'], ['Where was Abraham Mintchine born?', 'Kiev'], ['Where was John A. Dalles born?', 'Pittsburgh'], ['Where was Christian Day born?', 'Blackpool'], ['Where was Alfred Kossmann born?', 'Leiden'], ['Where was Victor Tolgesy born?', 'Hungary'], ['Where was Devin Hoff born?', 'Colorado'], ['Where was Linnea Johnson born?', 'Chicago'], ['Where was Brett Hestla born?', 'Alabama'], ['Where was Armand Toussaint born?', 'Paris'], ['Where was George Johnston born?', 'Melbourne'], ['Where was Elvis Yero born?', 'Havana'], ['Where was Spyros Sofos born?', 'Athens'], ['Where was John Whistler born?', 'Ulster'], ['Where was William Emery born?', 'London'], ['Where was John Buckley born?', 'Leeds'], ['Where was Nigel Tangye born?', 'Kensington'], ['Where was Nicholas Colla born?', 'Melbourne'], ['Where was Chuck Swirsky born?', 'Norfolk'], ['Where was Milton Wright born?', 'Georgia'], ['Where was William Clarke born?', 'Nottingham'], ['Where was Rebecca Pronsky born?', 'Brooklyn'], ['Where was Raymond Monsour Scurfield born?', 'Chicago'], ['Where was Wendy Mulford born?', 'Wales'], ['Where was Luis Ulacia born?', 'Havana'], ['Where was Robert Briffault born?', 'London'], ['Where was Lou Stein born?', 'Philadelphia'], ['Where was George Savoidakis born?', 'Crete'], [\"Where was Laurent d'Arvieux born?\", 'Marseille'], ['Where was Neil Rollinson born?', 'Yorkshire'], ['Where was Gustav Bartholin Hagen born?', 'Copenhagen'], ['Where was Andy LaRocque born?', 'Gothenburg'], ['Where was John Hirst born?', 'Bradford'], ['Where was Gordon Stratton born?', 'Winnipeg'], ['Where was Sarah Stiles born?', 'Massachusetts'], ['Where was Piotr Buciarski born?', 'Warsaw'], ['Where was Robert Morrison, 1st Baron Morrison born?', 'Aberdeen'], ['Where was Neil Doncaster born?', 'Devon'], ['Where was David Solomon born?', 'Australia'], ['Where was James S. Smart born?', 'Baltimore'], ['Where was Thomas Passmore born?', 'Belfast'], ['Where was Nathan B. Spingold born?', 'Chicago'], ['Where was Emmanuel Scheffer born?', 'Germany'], ['Where was Charles Nicholas Aubé born?', 'Paris'], ['Where was Wilkes Angel born?', 'Exeter'], ['Where was Benjamin Kunkel born?', 'Colorado'], ['Where was Steve Bjorklund born?', 'Chicago'], ['Where was Branko %C5%A0alamon born?', 'Zagreb'], ['Where was J. N. Williamson born?', 'Indianapolis'], ['Where was George Butler born?', 'Mansfield'], ['Where was Tom McGuinness born?', 'Wimbledon'], ['Where was Owen McAuley born?', 'Belfast'], ['Where was Roger Q. Williams born?', 'Brooklyn'], ['Where was Gabriel Tschumi born?', 'Switzerland'], ['Where was Henry Hall born?', 'Sheffield'], ['Where was Rudolf von Erlach born?', 'Bern'], ['Where was Keena Rothhammer born?', 'Arkansas'], ['Where was John Ambrose Meyer born?', 'Baltimore'], ['Where was Jean Baptiste Rives born?', 'Bordeaux'], ['Where was Jacob Guay born?', 'Quebec'], ['Where was Ryan Naraine born?', 'Georgetown'], ['Where was Claude Wiseler born?', 'Luxembourg'], ['Where was Idriss Ndele Moussa born?', 'Chad'], ['Where was Meike Evers born?', 'Berlin'], ['Where was Ruslan Gritsan born?', 'Moscow'], ['Where was Ahmed Maher born?', 'Alexandria'], ['Where was Dion Fischer born?', 'Romeo'], ['Where was James Burnham born?', 'Chicago'], ['Where was Johannes Falkenberg born?', 'Oslo'], ['Where was Peter Milligan born?', 'London'], ['Where was Thomas Kerr born?', 'Calgary'], ['Where was Lou Angeli born?', 'Wilmington'], ['Where was Tom French born?', 'Kilkenny'], ['Where was Kex Gorin born?', 'Birmingham'], ['Where was Thomas Birch Florence born?', 'Philadelphia'], ['Where was Siddhant Karnick born?', 'Mumbai'], ['Where was Mel Watkins born?', 'Memphis'], ['Where was Warren Cann born?', 'Victoria'], ['Where was Henry Albert Roby born?', 'Massachusetts'], ['Where was William Worthy born?', 'Boston'], ['Where was Leslie Howe born?', 'Ontario'], ['Where was Oscar Bianchi born?', 'Milan'], ['Where was Michael Hosking born?', 'Singapore'], ['Where was Andrew Grant born?', 'Birmingham'], ['Where was Adelaide Tosi born?', 'Milan'], ['Where was James Hooker born?', 'Windsor'], ['Where was Oxana Narozniak born?', 'Germany'], ['Where was Charles Hylton Stewart born?', 'Chester'], ['Where was Nick Richmond born?', 'Garland'], ['Where was Münir Göle born?', 'Istanbul'], ['Where was James Poole born?', 'Watford'], ['Where was Francisco Cervantes de Salazar born?', 'Toledo'], ['Where was Renato Beghe born?', 'Illinois'], ['Where was Michelangelo Pisani di Massa e di Mormile born?', 'Naples'], ['Where was Dan Armon born?', 'Jerusalem'], ['Where was Edward Stransham born?', 'Oxford'], ['Where was Gerald McLaughlin born?', 'Newark'], ['Where was Cathy Busby born?', 'Toronto'], ['Where was Michael Mason born?', 'Ohio'], ['Where was Rob Heanley born?', 'Surrey'], ['Where was Alfons Martí Bauçà born?', 'Palma'], ['Where was Colin Slater born?', 'Bradford'], ['Where was Keagan Kang born?', 'Perth'], ['Where was Brian Hodgson born?', 'Liverpool'], ['Where was John Cole born?', 'Toronto'], ['Where was Harry South born?', 'Fulham'], ['Where was Judd Buchanan born?', 'Edmonton'], ['Where was Eric Kloss born?', 'Greenville'], ['Where was Jumoke Verissimo born?', 'Lagos'], ['Where was Kym Anderson born?', 'Adelaide'], ['Where was Thomas Miller born?', 'Norwich'], ['Where was Ethel Merston born?', 'London'], ['Where was Barry Palmer born?', 'England'], ['Where was Stephanie Andujar born?', 'Manhattan'], ['Where was Derek Murray born?', 'Dublin'], ['Where was Ted Grouya born?', 'Bucharest'], ['Where was Marinko Madžgalj born?', 'Belgrade'], ['Where was John J. Gorman born?', 'Minneapolis'], ['Where was William Magennis born?', 'Belfast'], ['Where was Ondřej Neff born?', 'Prague'], ['Where was Andrea Newman born?', 'Dover'], [\"Where was Eugene O'Conor born?\", 'Ireland'], ['Where was Andrew Knight born?', 'England'], ['Where was Frederick Franklin Schrader born?', 'Hamburg'], ['Where was Janek Schaefer born?', 'England'], ['Where was Charles William King born?', 'Newport'], ['Where was Chris Woodhead born?', 'Edmonton'], ['Where was Stephen Tompkins born?', 'Cleveland'], ['Where was Arto Sipinen born?', 'Helsinki'], ['Where was Adam Begley born?', 'Boston'], ['Where was Pavel Polakovič born?', 'Slovakia'], ['Where was Tansy Davies born?', 'Bristol'], ['Where was Gregg Weaver born?', 'California'], ['Where was Liza Manili born?', 'Strasbourg'], ['Where was Jason Eckardt born?', 'Princeton'], ['Where was Jon Elster born?', 'Oslo'], ['Where was Fyodor Vinberg born?', 'Kiev'], ['Where was Ali Lakhani born?', 'England'], ['Where was Alfred Payne born?', 'Leicester'], ['Where was Sean McGreevy born?', 'Belfast'], ['Where was Christine Niederberger Betton born?', 'Bordeaux'], ['Where was Anna Maria Guarnieri born?', 'Milan'], ['Where was Gerald J. Tate born?', 'Belfast'], ['Where was Henry Howell born?', 'Richmond'], ['Where was Baruch Arensburg born?', 'Santiago'], ['Where was Hedvig Willman born?', 'Stockholm'], ['Where was Jane Tanner born?', 'Melbourne'], ['Where was John Ferrar Holms born?', 'India'], ['Where was Myer Hoffman born?', 'Leeds'], ['Where was Joan Hartigan born?', 'Sydney'], ['Where was Dionysis Boukouvalas born?', 'Athens'], ['Where was Galina Fokina born?', 'Moscow'], ['Where was Dexter Perkins born?', 'Boston'], [\"Where was Philippe N'Dioro born?\", 'France'], ['Where was William Mattieu Williams born?', 'London'], ['Where was Sara Agnes Mclaughlin Conboy born?', 'Boston'], ['Where was Adam Melonas born?', 'Canberra'], ['Where was Hifumi Shimoyama born?', 'Japan'], ['Where was Claire Cox born?', 'Peterborough'], ['Where was Manu Farrarons born?', 'France'], ['Where was Frank Gohlke born?', 'Texas'], ['Where was Paul Chadwick born?', 'Seattle'], ['Where was Vlastimil Pt%C3%A1k born?', 'Prague'], ['Where was Huck Whitney born?', 'Birmingham'], ['Where was Richard Nye born?', 'Gloucester'], ['Where was Bill Downe born?', 'Montreal'], ['Where was Lee King born?', 'Dublin'], ['Where was Gladys Ewart born?', 'Ottawa'], ['Where was Stephen Griew born?', 'London'], ['Where was Michel Reis born?', 'Luxembourg'], ['Where was Anselm Jappe born?', 'Bonn'], ['Where was Freddy Winnai born?', 'Philadelphia'], ['Where was Edward Harrington born?', 'Victoria'], ['Where was Harry James Angus born?', 'Melbourne'], ['Where was David J. Farrar born?', 'London'], ['Where was John Smith born?', 'Toronto'], ['Where was Johnny Rodgers born?', 'Miami'], ['Where was Stacy Barthe born?', 'Brooklyn'], ['Where was Ákos Császár born?', 'Budapest'], ['Where was Glynnis McDaris born?', 'Memphis'], ['Where was Johann Friedrich Schleusner born?', 'Leipzig'], ['Where was Michael Swanton born?', 'London'], ['Where was Kari Økland born?', 'Bergen'], ['Where was Rosabelle Sinclair born?', 'Russia'], ['Where was Thomas Smith born?', 'England'], ['Where was Mickael Marquet born?', 'France'], ['Where was Michael Upton born?', 'Birmingham'], ['Where was Sally Price born?', 'Boston'], ['Where was Luca Bottale born?', 'Milan'], ['Where was Paul Moyer Limbert born?', 'Pennsylvania'], ['Where was Malcolm Terris born?', 'Sunderland'], ['Where was Mia Riddle born?', 'Ventura'], ['Where was Irina Borogan born?', 'Moscow'], ['Where was Aleksander Kogoj born?', 'Ljubljana'], ['Where was Thomas Edward Gordon born?', 'Aberdeen'], ['Where was Wilhelm Krause born?', 'Hanover'], ['Where was Albrecht Dietz born?', 'Dresden'], ['Where was Grigori Kromanov born?', 'Tallinn'], ['Where was Sami Hinkka born?', 'Finland'], ['Where was Paul Burston born?', 'Yorkshire'], ['Where was William Francis Whitman, Jr. born?', 'Chicago'], ['Where was John D. Strong born?', 'Lawrence'], ['Where was Aleksandar Simi%C4%87 born?', 'Belgrade'], ['Where was Murray Hocking born?', 'Victoria'], ['Where was Darren Ockert born?', 'Lincoln'], ['Where was Max Lehmann born?', 'Berlin'], ['Where was Riccardo Ghedin born?', 'Rome'], ['Where was Pietro Fancelli born?', 'Bologna'], ['Where was Henry Frederick Strohecker born?', 'Macon'], ['Where was George J. Graham, Jr. born?', 'Dayton'], ['Where was Frederick Hickford born?', 'Brunswick'], ['Where was Leah Goldstein born?', 'Canada'], ['Where was Louis Gurlitt born?', 'Holstein'], ['Where was Tom Luken born?', 'Cincinnati'], ['Where was Annar Petersen born?', 'Oslo'], ['Where was Behnoosh Tabatabayi born?', 'Tehran'], ['Where was Vernon Carroll Porter born?', 'Cleveland'], ['Where was Ernest Breton born?', 'Paris'], ['Where was Andrew Lemoncello born?', 'Tokyo'], ['Where was Myra Sklarew born?', 'Baltimore'], ['Where was John Rutledge, Jr. born?', 'Charleston'], ['Where was Ashley Webster born?', 'Brighton'], ['Where was Matthew T. Abruzzo born?', 'Brooklyn'], ['Where was Erik Willoch born?', 'Oslo'], ['Where was Heinz Oestergaard born?', 'Berlin'], ['Where was Terry Byrne born?', 'London'], ['Where was Jared Cohen born?', 'Weston'], ['Where was Alma Redlinger born?', 'Bucharest'], ['Where was John Millen born?', 'Savannah'], ['Where was Julien Fountain born?', 'Sussex'], ['Where was Giuseppe de Majo born?', 'Naples'], ['Where was Joel Pelletier born?', 'Massachusetts'], ['Where was Rudolf Ehlers born?', 'Hamburg'], ['Where was Jacek Sauk born?', 'Vilnius'], ['Where was Stephen Luttrell born?', 'Leicester'], ['Where was Lawrence Walford born?', 'London'], ['Where was Stella Inger born?', 'Russia'], ['Where was Ludwig von Buhl born?', 'Munich'], ['Where was Shen Zhihua born?', 'Beijing'], ['Where was Fenton Johnson born?', 'Chicago'], ['Where was Sasha Neulinger born?', 'Philadelphia'], ['Where was Gerald Murphy born?', 'Boston'], ['Where was Alana Miller born?', 'Winnipeg'], ['Where was Drew McDowall born?', 'Paisley'], ['Where was Henry Corbould born?', 'London'], ['Where was Ymer Pampuri born?', 'Tirana'], ['Where was Eduard Ritter von Weber born?', 'Munich'], ['Where was Raymond R. Schumacher born?', 'Chicago'], ['Where was Pierre Sabatie born?', 'France'], ['Where was Nariman Farvardin born?', 'Iran'], ['Where was Jorn Madslien born?', 'Oslo'], ['Where was Mary Jayne Gold born?', 'Chicago'], ['Where was Nelson Fogarty born?', 'Canterbury'], ['Where was John Caskie born?', 'Richmond'], ['Where was Gillian Cooke born?', 'Edinburgh'], ['Where was Joseph Silverstein born?', 'Detroit'], ['Where was Ernest Lafont born?', 'Lyon'], ['Where was David T. Walker born?', 'Tulsa'], ['Where was Jessy Moss born?', 'England'], ['Where was Catherine Ransom Karoly born?', 'Minneapolis'], ['Where was Akihiro Tsukiyama born?', 'Tokyo'], ['Where was Ferenc A. Váli born?', 'Budapest'], ['Where was William Williams born?', 'Bolton'], ['Where was John Mann Goggin born?', 'Chicago'], [\"Where was Hélène Carrère d'Encausse born?\", 'Paris'], ['Where was Bill Dillard born?', 'Pennsylvania'], ['Where was Richard Schroeppel born?', 'Illinois'], ['Where was Mark McCulloch born?', 'Inverness'], ['Where was Richard Symonds born?', 'Norfolk'], ['Where was Nasos Galakteros born?', 'Athens'], [\"Where was Joe M. O'Connell born?\", 'Austin'], ['Where was Brody Condon born?', 'Mexico'], ['Where was Ernst Zacharias Platner born?', 'Leipzig'], ['Where was Henri Nussbaumer born?', 'Paris'], ['Where was John Boson born?', 'Paul'], ['Where was Morris Bear Squire born?', 'Chicago'], ['Where was Yolande Thibeault born?', 'Montreal'], ['Where was James Tocco born?', 'Detroit'], ['Where was Constant Ferdinand Burille born?', 'Paris'], ['Where was John Samuel Kenyon born?', 'Medina'], ['Where was Franz Galich born?', 'Guatemala'], ['Where was Tom Adams born?', 'Gettysburg'], ['Where was Daniel Joseph Jaffé born?', 'London'], ['Where was Matteo Salvini born?', 'Milan'], ['Where was Tony Naumovski born?', 'Sydney'], ['Where was Joseph M. Baumgarten born?', 'Vienna'], ['Where was Daniel Sproule born?', 'Melbourne'], ['Where was Theodor Holman born?', 'Amsterdam'], ['Where was Dave Connell born?', 'Dublin'], ['Where was Alfredo Moreno born?', 'Santiago'], ['Where was Luke Sullivan born?', 'Singapore'], ['Where was Lynn Faulds Wood born?', 'Glasgow'], ['Where was Roy Stuart Brown born?', 'Minneapolis'], ['Where was Andrew Park born?', 'Lafayette'], ['Where was Richard Laurence born?', 'Bath'], ['Where was Brian Murphy born?', 'Ottawa'], ['Where was René Mayer born?', 'Paris'], ['Where was Leopoldo Gout born?', 'Mexico'], ['Where was Robert Tripp Ross born?', 'Washington'], ['Where was Ragnhild Lundén born?', 'Gothenburg'], ['Where was Fatima Rainey born?', 'Sweden'], ['Where was John Edwin born?', 'London'], ['Where was Arthur Bingham Walkley born?', 'Bristol'], ['Where was Mazen Mneimneh born?', 'Lebanon'], ['Where was Howard O. McMahon born?', 'Alberta'], ['Where was Joe Caccia born?', 'Naples'], ['Where was George Stone born?', 'London'], ['Where was Alice Temple born?', 'London'], ['Where was Andrew Kennedy born?', 'Dayton'], ['Where was Michael Randle born?', 'England'], ['Where was Michael Charlton born?', 'Sydney'], ['Where was Lars Svensson born?', 'Stockholm'], ['Where was Eva Aridjis born?', 'Holland'], ['Where was David B. Mellish born?', 'Oxford'], ['Where was John Brown born?', 'Sheffield'], ['Where was Patrick Hicks born?', 'Charlotte'], ['Where was Ludovicus Stornebrink born?', 'Rotterdam'], ['Where was Michael Feiner born?', 'Gothenburg'], ['Where was Giancarlo Primo born?', 'Italy'], ['Where was Thomas Davis born?', 'Worcester'], ['Where was Lottie Gilson born?', 'Pennsylvania'], ['Where was Brian Bram born?', 'Chicago'], ['Where was J. E. P. Aldous born?', 'Sheffield'], ['Where was Billy Blue born?', 'Jamaica'], ['Where was Christian Wilhelm Berger born?', 'Bucharest'], ['Where was Ulla Pia born?', 'Copenhagen'], ['Where was Samuel Bernstein born?', 'France'], ['Where was Steve Laine born?', 'London'], ['Where was Antonio Ruiz de Montoya born?', 'Lima'], ['Where was Bernhard R%C3%BChling born?', 'Stuttgart'], ['Where was Don Swaim born?', 'Kansas'], ['Where was Lawrence Dundas, 1st Earl of Zetland born?', 'Westminster'], ['Where was Sam Bartlett born?', 'Burlington'], ['Where was Martin Timell born?', 'Stockholm'], ['Where was Lucien Barbour born?', 'Canton'], ['Where was Edward Moore born?', 'Cardiff'], ['Where was Bridget Allchin born?', 'Oxford'], ['Where was Pieter Harting born?', 'Rotterdam'], ['Where was Anthony Poon born?', 'Singapore'], ['Where was Brihaspati Dev Triguna born?', 'India'], ['Where was Knut Kjeldstadli born?', 'Oslo'], ['Where was William Byrne born?', 'London'], ['Where was Fiachna %C3%93 Braon%C3%A1in born?', 'Dublin'], ['Where was Harold Drasdo born?', 'Bradford'], ['Where was William Main Page born?', 'London'], ['Where was Theron Metcalf born?', 'Franklin'], ['Where was Robert Callender born?', 'Barbados'], ['Where was Mohd Shoaib Hassan born?', 'Lahore'], ['Where was Lisa Sheridan born?', 'Macon'], ['Where was Georg Kloss born?', 'Frankfurt'], ['Where was Julianna Barwick born?', 'Louisiana'], ['Where was William Erskine born?', 'Edinburgh'], ['Where was Albert Chamberland born?', 'Montreal'], ['Where was Len Zengel born?', 'Dayton'], ['Where was William Henry Brockenbrough born?', 'Virginia'], ['Where was Brook Pridemore born?', 'Detroit'], ['Where was Arthur Levering born?', 'Baltimore'], ['Where was Sheila Sondergard born?', 'Seoul'], ['Where was Lee Davies born?', 'Blackburn'], ['Where was Nicole Richardson born?', 'Melbourne'], ['Where was Paul Heeren born?', 'Adelaide'], ['Where was William James Henderson born?', 'Newark'], ['Where was Nicolae Herlea born?', 'Bucharest'], ['Where was Fred Rosenstock born?', 'Galicia'], ['Where was Lucie Vrbensk%C3%A1 born?', 'Prague'], ['Where was Kenneth William Junor born?', 'Toronto'], ['Where was Nicolas Colibert born?', 'Paris'], ['Where was Jan Pawe%C5%82 Nowacki born?', 'Berlin'], ['Where was Brandon Bird born?', 'Carmichael'], ['Where was A. M. Esmonde born?', 'Swansea'], ['Where was Thomas Wright Rudderow born?', 'Philadelphia'], ['Where was Francis Gano Benedict born?', 'Milwaukee'], ['Where was William Davidson born?', 'Charleston'], ['Where was Austin Lloyd Fleming born?', 'Toronto'], ['Where was Li Weiliang born?', 'Beijing'], ['Where was Migidio Bourifa born?', 'Casablanca'], ['Where was Zhang Li born?', 'Shanghai'], ['Where was Charles Jones born?', 'Wolverhampton'], ['Where was Yehonatan Berick born?', 'Israel'], ['Where was Georg Hellmesberger born?', 'Vienna'], ['Where was George Georgiou born?', 'London'], ['Where was John R. Tunis born?', 'Boston'], [\"Where was Buono de' Buoni born?\", 'Naples'], ['Where was Dave Steele born?', 'Tampa'], ['Where was Tim Benjamin born?', 'London'], ['Where was Alina Gut born?', 'Lublin'], ['Where was Michel Roger Lafosse born?', 'Belgium'], ['Where was Martin A. Janis born?', 'Toledo'], ['Where was John William Mellor born?', 'London'], ['Where was Robert S. Richardson born?', 'Indiana'], ['Where was Derek Kreckler born?', 'Sydney'], ['Where was Yosef Harish born?', 'Jerusalem'], ['Where was Robert Chen born?', 'Taipei'], ['Where was Max Angelelli born?', 'Bologna'], ['Where was Alan Welsh born?', 'Edinburgh'], ['Where was Billy Edwards born?', 'Birmingham'], ['Where was Paul Lebreton born?', 'Bordeaux'], ['Where was Lowrell Simon born?', 'Chicago'], ['Where was R. W. Johnson born?', 'England'], ['Where was Benjamin Pine born?', 'London'], ['Where was Charles Forbes René de Montalembert born?', 'London'], ['Where was George Schutt born?', 'Ireland'], ['Where was Matt Ryan born?', 'Sydney'], ['Where was Marie Nicolas Sylvestre Guillon born?', 'Paris'], ['Where was Daniel Spielman born?', 'Philadelphia'], ['Where was Tony Conran born?', 'India'], [\"Where was Oscar O'Brien born?\", 'Ottawa'], ['Where was Pietra Montecorvino born?', 'Naples'], ['Where was Lennox Raphael born?', 'Trinidad'], ['Where was Su Lian Tan born?', 'Malaysia'], ['Where was Oğuz Abadan born?', 'Ankara'], ['Where was Donald W. Loveland born?', 'Rochester'], ['Where was Francis MacManus born?', 'Kilkenny'], ['Where was Monica Esposito born?', 'Genoa'], ['Where was Mike Vosburg born?', 'California'], ['Where was Kraisak Choonhavan born?', 'Bangkok'], ['Where was Ken Lyons born?', 'Florida'], ['Where was René Renoult born?', 'Paris'], ['Where was David Empringham born?', 'Toronto'], ['Where was Saleem Pervez born?', 'Lahore'], ['Where was Adnan Awad born?', 'Palestine'], ['Where was Juan Carlos Bersague born?', 'Havana'], ['Where was Joseph Bosworth born?', 'Derbyshire'], ['Where was Khalid Malik born?', 'Pakistan'], ['Where was Paddy Crowley born?', 'Dublin'], ['Where was John Joseph Braham, Sr. born?', 'England'], ['Where was Calvary Morris born?', 'Charleston'], ['Where was Carlo Bacchiocco born?', 'Milan'], ['Where was Robert Ilyasov born?', 'Kazan'], ['Where was George Henderson born?', 'Edinburgh'], ['Where was Aaron Walden born?', 'Warsaw']]\n","['London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London', 'London']\n","Correct: 25.0 out of 500.0: 5.0%\n"]}],"source":["!python /content/drive/MyDrive/speech_research/lecture/Assignment5_2021/student-new/src/london_baseline.py --eval_corpus_path birth_dev.tsv"]},{"cell_type":"markdown","metadata":{"id":"wLLhxNjh0H6Z"},"source":["### Run Pretrain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1499,"status":"ok","timestamp":1635407341443,"user":{"displayName":"오승현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08073038130517070928"},"user_tz":-540},"id":"KZGBc7PR3m_r","outputId":"4ed93243-bfe6-439c-dfdb-c2320f804aad"},"outputs":[{"name":"stdout","output_type":"stream","text":["data has 418352 characters, 256 unique.\n","x: Khatchig Mouradian. Khatchig Mouradian ⁇st, writer and translator born in Lebanon .⁇is a journali□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: hatchig Mouradian. Khatchig Mouradian ⁇st, writer and translator born in Lebanon .⁇is a journali□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: Jacob Henry Stude⁇ Studer⁇r. Jacob Henry□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: acob Henry Stude⁇ Studer⁇r. Jacob Henry□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: John Stephen. Born in Glasgow, Stephen became a welder's ap⁇ool .⁇prentice on leaving sch□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: ohn Stephen. Born in Glasgow, Stephen became a welder's ap⁇ool .⁇prentice on leaving sch□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: Ge⁇na Willis⁇orgi□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: e⁇na Willis⁇orgi□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n"]}],"source":["!python /content/drive/MyDrive/speech_research/lecture/a5_2020/student-new/src/dataset.py charcorruption"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2201168,"status":"ok","timestamp":1635412971683,"user":{"displayName":"오승현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08073038130517070928"},"user_tz":-540},"id":"BgexX6Gr0MD3","outputId":"8572f6f3-ed25-4229-b899-7a297a411b03"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello World\n","wiki.txt\n","vanilla.pretrain.params\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","epoch 1 iter 22: train loss 4.53140. lr 5.967984e-06: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 2 iter 22: train loss 4.14357. lr 1.193597e-05: 100% 23/23 [00:08<00:00,  2.86it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 3 iter 22: train loss 3.83953. lr 1.790395e-05: 100% 23/23 [00:07<00:00,  2.88it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 4 iter 22: train loss 3.62053. lr 2.387194e-05: 100% 23/23 [00:07<00:00,  2.88it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 5 iter 22: train loss 3.41786. lr 2.983992e-05: 100% 23/23 [00:08<00:00,  2.86it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 6 iter 22: train loss 3.24906. lr 3.580790e-05: 100% 23/23 [00:08<00:00,  2.87it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 7 iter 22: train loss 3.12711. lr 4.177589e-05: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 8 iter 22: train loss 3.00714. lr 4.774387e-05: 100% 23/23 [00:08<00:00,  2.86it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 9 iter 22: train loss 2.89977. lr 5.371186e-05: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 10 iter 22: train loss 2.83869. lr 5.967984e-05: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 11 iter 22: train loss 2.77403. lr 6.564782e-05: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 12 iter 22: train loss 2.69827. lr 7.161581e-05: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 13 iter 22: train loss 2.65888. lr 7.758379e-05: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 14 iter 22: train loss 2.60788. lr 8.355178e-05: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 15 iter 22: train loss 2.57945. lr 8.951976e-05: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 16 iter 22: train loss 2.52876. lr 9.548774e-05: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 17 iter 22: train loss 2.52078. lr 1.014557e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 18 iter 22: train loss 2.50986. lr 1.074237e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 19 iter 22: train loss 2.49562. lr 1.133917e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 20 iter 22: train loss 2.46639. lr 1.193597e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 21 iter 22: train loss 2.41799. lr 1.253277e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 22 iter 22: train loss 2.38246. lr 1.312956e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 23 iter 22: train loss 2.39036. lr 1.372636e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 24 iter 22: train loss 2.37065. lr 1.432316e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 25 iter 22: train loss 2.34614. lr 1.491996e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 26 iter 22: train loss 2.32733. lr 1.551676e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 27 iter 22: train loss 2.28816. lr 1.611356e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 28 iter 22: train loss 2.28115. lr 1.671036e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 29 iter 22: train loss 2.22964. lr 1.730715e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 30 iter 22: train loss 2.23143. lr 1.790395e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 31 iter 22: train loss 2.20197. lr 1.850075e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 32 iter 22: train loss 2.18643. lr 1.909755e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 33 iter 22: train loss 2.17357. lr 1.969435e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 34 iter 22: train loss 2.15758. lr 2.029115e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 35 iter 22: train loss 2.11933. lr 2.088794e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 36 iter 22: train loss 2.09403. lr 2.148474e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 37 iter 22: train loss 2.06951. lr 2.208154e-04: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 38 iter 22: train loss 2.05762. lr 2.267834e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 39 iter 22: train loss 2.04821. lr 2.327514e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 40 iter 22: train loss 2.04514. lr 2.387194e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 41 iter 22: train loss 1.97023. lr 2.446873e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 42 iter 22: train loss 1.99813. lr 2.506553e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 43 iter 22: train loss 1.96623. lr 2.566233e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 44 iter 22: train loss 1.94855. lr 2.625913e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 45 iter 22: train loss 1.94657. lr 2.685593e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 46 iter 22: train loss 1.92485. lr 2.745273e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 47 iter 22: train loss 1.90501. lr 2.804952e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 48 iter 22: train loss 1.89223. lr 2.864632e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 49 iter 22: train loss 1.86602. lr 2.924312e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 50 iter 22: train loss 1.84758. lr 2.983992e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 51 iter 22: train loss 1.78360. lr 3.043672e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 52 iter 22: train loss 1.81303. lr 3.103352e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 53 iter 22: train loss 1.77480. lr 3.163032e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 54 iter 22: train loss 1.77413. lr 3.222711e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 55 iter 22: train loss 1.75909. lr 3.282391e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 56 iter 22: train loss 1.72379. lr 3.342071e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 57 iter 22: train loss 1.73019. lr 3.401751e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 58 iter 22: train loss 1.69223. lr 3.461431e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 59 iter 22: train loss 1.68659. lr 3.521111e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 60 iter 22: train loss 1.67457. lr 3.580790e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 61 iter 22: train loss 1.66257. lr 3.640470e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 62 iter 22: train loss 1.61210. lr 3.700150e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 63 iter 22: train loss 1.61742. lr 3.759830e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 64 iter 22: train loss 1.61141. lr 3.819510e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 65 iter 22: train loss 1.61991. lr 3.879190e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 66 iter 22: train loss 1.60213. lr 3.938869e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 67 iter 22: train loss 1.60275. lr 3.998549e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 68 iter 22: train loss 1.56901. lr 4.058229e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 69 iter 22: train loss 1.58288. lr 4.117909e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 70 iter 22: train loss 1.52271. lr 4.177589e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 71 iter 22: train loss 1.53735. lr 4.237269e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 72 iter 22: train loss 1.52134. lr 4.296948e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 73 iter 22: train loss 1.50711. lr 4.356628e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 74 iter 22: train loss 1.48491. lr 4.416308e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 75 iter 22: train loss 1.46381. lr 4.475988e-04: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 76 iter 22: train loss 1.42842. lr 4.535668e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 77 iter 22: train loss 1.45519. lr 4.595348e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 78 iter 22: train loss 1.46346. lr 4.655028e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 79 iter 22: train loss 1.47898. lr 4.714707e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 80 iter 22: train loss 1.42289. lr 4.774387e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 81 iter 22: train loss 1.46072. lr 4.834067e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 82 iter 22: train loss 1.39400. lr 4.893747e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 83 iter 22: train loss 1.35050. lr 4.953427e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 84 iter 22: train loss 1.41710. lr 5.013107e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 85 iter 22: train loss 1.38840. lr 5.072786e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 86 iter 22: train loss 1.37243. lr 5.132466e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 87 iter 22: train loss 1.36249. lr 5.192146e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 88 iter 22: train loss 1.34173. lr 5.251826e-04: 100% 23/23 [00:08<00:00,  2.80it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 89 iter 22: train loss 1.35207. lr 5.311506e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 90 iter 22: train loss 1.31748. lr 5.371186e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 91 iter 22: train loss 1.35643. lr 5.430865e-04: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 92 iter 22: train loss 1.35466. lr 5.490545e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 93 iter 22: train loss 1.29361. lr 5.550225e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 94 iter 22: train loss 1.29440. lr 5.609905e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 95 iter 22: train loss 1.30001. lr 5.669585e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 96 iter 22: train loss 1.31467. lr 5.729265e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 97 iter 22: train loss 1.27165. lr 5.788944e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 98 iter 22: train loss 1.30957. lr 5.848624e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 99 iter 22: train loss 1.29006. lr 5.908304e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 100 iter 22: train loss 1.25528. lr 5.967984e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 101 iter 22: train loss 1.28817. lr 6.027664e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 102 iter 22: train loss 1.27958. lr 6.087344e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 103 iter 22: train loss 1.24633. lr 6.147024e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 104 iter 22: train loss 1.25048. lr 6.206703e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 105 iter 22: train loss 1.23758. lr 6.266383e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 106 iter 22: train loss 1.23394. lr 6.326063e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 107 iter 22: train loss 1.19583. lr 6.385743e-04: 100% 23/23 [00:08<00:00,  2.80it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 108 iter 22: train loss 1.21480. lr 6.445423e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 109 iter 22: train loss 1.20576. lr 6.505103e-04: 100% 23/23 [00:08<00:00,  2.80it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 110 iter 22: train loss 1.22047. lr 6.564782e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 111 iter 22: train loss 1.18579. lr 6.624462e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 112 iter 22: train loss 1.23138. lr 6.684142e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 113 iter 22: train loss 1.21011. lr 6.743822e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 114 iter 22: train loss 1.17130. lr 6.803502e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 115 iter 22: train loss 1.17559. lr 6.863182e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 116 iter 22: train loss 1.16287. lr 6.922861e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 117 iter 22: train loss 1.16247. lr 6.982541e-04: 100% 23/23 [00:08<00:00,  2.80it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 118 iter 22: train loss 1.13832. lr 7.042221e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 119 iter 22: train loss 1.15038. lr 7.101901e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 120 iter 22: train loss 1.12247. lr 7.161581e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 121 iter 22: train loss 1.15353. lr 7.221261e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 122 iter 22: train loss 1.13549. lr 7.280940e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 123 iter 22: train loss 1.13082. lr 7.340620e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 124 iter 22: train loss 1.11347. lr 7.400300e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 125 iter 22: train loss 1.13659. lr 7.459980e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 126 iter 22: train loss 1.09340. lr 7.519660e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 127 iter 22: train loss 1.11103. lr 7.579340e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 128 iter 22: train loss 1.12793. lr 7.639020e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 129 iter 22: train loss 1.08573. lr 7.698699e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 130 iter 22: train loss 1.09856. lr 7.758379e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 131 iter 22: train loss 1.08741. lr 7.818059e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 132 iter 22: train loss 1.08047. lr 7.877739e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 133 iter 22: train loss 1.08584. lr 7.937419e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 134 iter 22: train loss 1.12720. lr 7.997099e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 135 iter 22: train loss 1.11852. lr 8.056778e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 136 iter 22: train loss 1.09413. lr 8.116458e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 137 iter 22: train loss 1.05212. lr 8.176138e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 138 iter 22: train loss 1.07789. lr 8.235818e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 139 iter 22: train loss 1.04776. lr 8.295498e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 140 iter 22: train loss 1.04278. lr 8.355178e-04: 100% 23/23 [00:08<00:00,  2.80it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 141 iter 22: train loss 1.03927. lr 8.414857e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 142 iter 22: train loss 1.03678. lr 8.474537e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 143 iter 22: train loss 1.03026. lr 8.534217e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 144 iter 22: train loss 1.01817. lr 8.593897e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 145 iter 22: train loss 1.04617. lr 8.653577e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 146 iter 22: train loss 0.99410. lr 8.713257e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 147 iter 22: train loss 1.02958. lr 8.772936e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 148 iter 22: train loss 1.00593. lr 8.832616e-04: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 149 iter 22: train loss 1.04254. lr 8.892296e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 150 iter 22: train loss 0.98248. lr 8.951976e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 151 iter 22: train loss 1.03296. lr 9.011656e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 152 iter 22: train loss 0.99943. lr 9.071336e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 153 iter 22: train loss 1.01615. lr 9.131016e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 154 iter 22: train loss 1.03301. lr 9.190695e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 155 iter 22: train loss 0.98483. lr 9.250375e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 156 iter 22: train loss 1.00821. lr 9.310055e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 157 iter 22: train loss 1.01544. lr 9.369735e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 158 iter 22: train loss 0.98694. lr 9.429415e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 159 iter 22: train loss 0.98173. lr 9.489095e-04: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 160 iter 22: train loss 0.99899. lr 9.548774e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 161 iter 22: train loss 0.97512. lr 9.608454e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 162 iter 22: train loss 0.97546. lr 9.668134e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 163 iter 22: train loss 0.98155. lr 9.727814e-04: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 164 iter 22: train loss 0.98906. lr 9.787494e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 165 iter 22: train loss 0.92776. lr 9.847174e-04: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 166 iter 22: train loss 0.98204. lr 9.906853e-04: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 167 iter 22: train loss 0.97754. lr 9.966533e-04: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 168 iter 22: train loss 1.01700. lr 1.002621e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 169 iter 22: train loss 0.97961. lr 1.008589e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 170 iter 22: train loss 0.95736. lr 1.014557e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 171 iter 22: train loss 0.95175. lr 1.020525e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 172 iter 22: train loss 0.92773. lr 1.026493e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 173 iter 22: train loss 0.93629. lr 1.032461e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 174 iter 22: train loss 0.93148. lr 1.038429e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 175 iter 22: train loss 0.92928. lr 1.044397e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 176 iter 22: train loss 0.98830. lr 1.050365e-03: 100% 23/23 [00:08<00:00,  2.86it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 177 iter 22: train loss 0.95620. lr 1.056333e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 178 iter 22: train loss 0.91439. lr 1.062301e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 179 iter 22: train loss 0.94621. lr 1.068269e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 180 iter 22: train loss 0.92357. lr 1.074237e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 181 iter 22: train loss 0.94421. lr 1.080205e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 182 iter 22: train loss 0.93155. lr 1.086173e-03: 100% 23/23 [00:08<00:00,  2.80it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 183 iter 22: train loss 0.96205. lr 1.092141e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 184 iter 22: train loss 0.91795. lr 1.098109e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 185 iter 22: train loss 0.90412. lr 1.104077e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 186 iter 22: train loss 0.94037. lr 1.110045e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 187 iter 22: train loss 0.89419. lr 1.116013e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 188 iter 22: train loss 0.90273. lr 1.121981e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 189 iter 22: train loss 0.90030. lr 1.127949e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 190 iter 22: train loss 0.89384. lr 1.133917e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 191 iter 22: train loss 0.91772. lr 1.139885e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 192 iter 22: train loss 0.87042. lr 1.145853e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 193 iter 22: train loss 0.92519. lr 1.151821e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 194 iter 22: train loss 0.90266. lr 1.157789e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 195 iter 22: train loss 0.87929. lr 1.163757e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 196 iter 22: train loss 0.92279. lr 1.169725e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 197 iter 22: train loss 0.89917. lr 1.175693e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 198 iter 22: train loss 0.90709. lr 1.181661e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 199 iter 22: train loss 0.90648. lr 1.187629e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 200 iter 22: train loss 0.90783. lr 1.193597e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 201 iter 22: train loss 0.87716. lr 1.199565e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 202 iter 22: train loss 0.90081. lr 1.205533e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 203 iter 22: train loss 0.85463. lr 1.211501e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 204 iter 22: train loss 0.88721. lr 1.217469e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 205 iter 22: train loss 0.85708. lr 1.223437e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 206 iter 22: train loss 0.90005. lr 1.229405e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 207 iter 22: train loss 0.84837. lr 1.235373e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 208 iter 22: train loss 0.87641. lr 1.241341e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 209 iter 22: train loss 0.88059. lr 1.247309e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 210 iter 22: train loss 0.88439. lr 1.253277e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 211 iter 22: train loss 0.85221. lr 1.259245e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 212 iter 22: train loss 0.84880. lr 1.265213e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 213 iter 22: train loss 0.87176. lr 1.271181e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 214 iter 22: train loss 0.88313. lr 1.277149e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 215 iter 22: train loss 0.86287. lr 1.283117e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 216 iter 22: train loss 0.84270. lr 1.289085e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 217 iter 22: train loss 0.83273. lr 1.295053e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 218 iter 22: train loss 0.82875. lr 1.301021e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 219 iter 22: train loss 0.85031. lr 1.306988e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 220 iter 22: train loss 0.85852. lr 1.312956e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 221 iter 22: train loss 0.86143. lr 1.318924e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 222 iter 22: train loss 0.88220. lr 1.324892e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 223 iter 22: train loss 0.84957. lr 1.330860e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 224 iter 22: train loss 0.85607. lr 1.336828e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 225 iter 22: train loss 0.88512. lr 1.342796e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 226 iter 22: train loss 0.85782. lr 1.348764e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 227 iter 22: train loss 0.84175. lr 1.354732e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 228 iter 22: train loss 0.84846. lr 1.360700e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 229 iter 22: train loss 0.83838. lr 1.366668e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 230 iter 22: train loss 0.82125. lr 1.372636e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 231 iter 22: train loss 0.83327. lr 1.378604e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 232 iter 22: train loss 0.85223. lr 1.384572e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 233 iter 22: train loss 0.80105. lr 1.390540e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 234 iter 22: train loss 0.82891. lr 1.396508e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 235 iter 22: train loss 0.83277. lr 1.402476e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 236 iter 22: train loss 0.81488. lr 1.408444e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 237 iter 22: train loss 0.78452. lr 1.414412e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 238 iter 22: train loss 0.81191. lr 1.420380e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 239 iter 22: train loss 0.81670. lr 1.426348e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 240 iter 22: train loss 0.80204. lr 1.432316e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 241 iter 22: train loss 0.82440. lr 1.438284e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 242 iter 22: train loss 0.81040. lr 1.444252e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 243 iter 22: train loss 0.82968. lr 1.450220e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 244 iter 22: train loss 0.82062. lr 1.456188e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 245 iter 22: train loss 0.77693. lr 1.462156e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 246 iter 22: train loss 0.80680. lr 1.468124e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 247 iter 22: train loss 0.79030. lr 1.474092e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 248 iter 22: train loss 0.80138. lr 1.480060e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 249 iter 22: train loss 0.77971. lr 1.486028e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 250 iter 22: train loss 0.79438. lr 1.491996e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 251 iter 22: train loss 0.83516. lr 1.497964e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 252 iter 22: train loss 0.80651. lr 1.503932e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 253 iter 22: train loss 0.80009. lr 1.509900e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 254 iter 22: train loss 0.80495. lr 1.515868e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 255 iter 22: train loss 0.78393. lr 1.521836e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 256 iter 22: train loss 0.80170. lr 1.527804e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 257 iter 22: train loss 0.79371. lr 1.533772e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 258 iter 22: train loss 0.80288. lr 1.539740e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 259 iter 22: train loss 0.80536. lr 1.545708e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 260 iter 22: train loss 0.80770. lr 1.551676e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 261 iter 22: train loss 0.81199. lr 1.557644e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 262 iter 22: train loss 0.79115. lr 1.563612e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 263 iter 22: train loss 0.76264. lr 1.569580e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 264 iter 22: train loss 0.75149. lr 1.575548e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 265 iter 22: train loss 0.79135. lr 1.581516e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 266 iter 22: train loss 0.79663. lr 1.587484e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 267 iter 22: train loss 0.73626. lr 1.593452e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 268 iter 22: train loss 0.79515. lr 1.599420e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 269 iter 22: train loss 0.75103. lr 1.605388e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 270 iter 22: train loss 0.76573. lr 1.611356e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 271 iter 22: train loss 0.76659. lr 1.617324e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 272 iter 22: train loss 0.79962. lr 1.623292e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 273 iter 22: train loss 0.82105. lr 1.629260e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 274 iter 22: train loss 0.77532. lr 1.635228e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 275 iter 22: train loss 0.77866. lr 1.641196e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 276 iter 22: train loss 0.76299. lr 1.647164e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 277 iter 22: train loss 0.77822. lr 1.653132e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 278 iter 22: train loss 0.76088. lr 1.659100e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 279 iter 22: train loss 0.76784. lr 1.665068e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 280 iter 22: train loss 0.78825. lr 1.671036e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 281 iter 22: train loss 0.74992. lr 1.677004e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 282 iter 22: train loss 0.77162. lr 1.682971e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 283 iter 22: train loss 0.76469. lr 1.688939e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 284 iter 22: train loss 0.74305. lr 1.694907e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 285 iter 22: train loss 0.79723. lr 1.700875e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 286 iter 22: train loss 0.75836. lr 1.706843e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 287 iter 22: train loss 0.75694. lr 1.712811e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 288 iter 22: train loss 0.78561. lr 1.718779e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 289 iter 22: train loss 0.73851. lr 1.724747e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 290 iter 22: train loss 0.74247. lr 1.730715e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 291 iter 22: train loss 0.74674. lr 1.736683e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 292 iter 22: train loss 0.74356. lr 1.742651e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 293 iter 22: train loss 0.71784. lr 1.748619e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 294 iter 22: train loss 0.75079. lr 1.754587e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 295 iter 22: train loss 0.73259. lr 1.760555e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 296 iter 22: train loss 0.75959. lr 1.766523e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 297 iter 22: train loss 0.77712. lr 1.772491e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 298 iter 22: train loss 0.78928. lr 1.778459e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 299 iter 22: train loss 0.73578. lr 1.784427e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 300 iter 22: train loss 0.75471. lr 1.790395e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 301 iter 22: train loss 0.74305. lr 1.796363e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 302 iter 22: train loss 0.74653. lr 1.802331e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 303 iter 22: train loss 0.75189. lr 1.808299e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 304 iter 22: train loss 0.74309. lr 1.814267e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 305 iter 22: train loss 0.70746. lr 1.820235e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 306 iter 22: train loss 0.74322. lr 1.826203e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 307 iter 22: train loss 0.73675. lr 1.832171e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 308 iter 22: train loss 0.71124. lr 1.838139e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 309 iter 22: train loss 0.72553. lr 1.844107e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 310 iter 22: train loss 0.73872. lr 1.850075e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 311 iter 22: train loss 0.72726. lr 1.856043e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 312 iter 22: train loss 0.72170. lr 1.862011e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 313 iter 22: train loss 0.76745. lr 1.867979e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 314 iter 22: train loss 0.71878. lr 1.873947e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 315 iter 22: train loss 0.74616. lr 1.879915e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 316 iter 22: train loss 0.70547. lr 1.885883e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 317 iter 22: train loss 0.74503. lr 1.891851e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 318 iter 22: train loss 0.70335. lr 1.897819e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 319 iter 22: train loss 0.70945. lr 1.903787e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 320 iter 22: train loss 0.73702. lr 1.909755e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 321 iter 22: train loss 0.74937. lr 1.915723e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 322 iter 22: train loss 0.74116. lr 1.921691e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 323 iter 22: train loss 0.72517. lr 1.927659e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 324 iter 22: train loss 0.71580. lr 1.933627e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 325 iter 22: train loss 0.68435. lr 1.939595e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 326 iter 22: train loss 0.70657. lr 1.945563e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 327 iter 22: train loss 0.75066. lr 1.951531e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 328 iter 22: train loss 0.71357. lr 1.957499e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 329 iter 22: train loss 0.75636. lr 1.963467e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 330 iter 22: train loss 0.71443. lr 1.969435e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 331 iter 22: train loss 0.72541. lr 1.975403e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 332 iter 22: train loss 0.71633. lr 1.981371e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 333 iter 22: train loss 0.75059. lr 1.987339e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 334 iter 22: train loss 0.74985. lr 1.993307e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 335 iter 22: train loss 0.74563. lr 1.999275e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 336 iter 22: train loss 0.70446. lr 2.005243e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 337 iter 22: train loss 0.70177. lr 2.011211e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 338 iter 22: train loss 0.70222. lr 2.017179e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 339 iter 22: train loss 0.69773. lr 2.023147e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 340 iter 22: train loss 0.70262. lr 2.029115e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 341 iter 22: train loss 0.72617. lr 2.035083e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 342 iter 22: train loss 0.70097. lr 2.041051e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 343 iter 22: train loss 0.68005. lr 2.047019e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 344 iter 22: train loss 0.72323. lr 2.052986e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 345 iter 22: train loss 0.68726. lr 2.058954e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 346 iter 22: train loss 0.70709. lr 2.064922e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 347 iter 22: train loss 0.71376. lr 2.070890e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 348 iter 22: train loss 0.73699. lr 2.076858e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 349 iter 22: train loss 0.70399. lr 2.082826e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 350 iter 22: train loss 0.72581. lr 2.088794e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 351 iter 22: train loss 0.70193. lr 2.094762e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 352 iter 22: train loss 0.69206. lr 2.100730e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 353 iter 22: train loss 0.73330. lr 2.106698e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 354 iter 22: train loss 0.72404. lr 2.112666e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 355 iter 22: train loss 0.70994. lr 2.118634e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 356 iter 22: train loss 0.72677. lr 2.124602e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 357 iter 22: train loss 0.71690. lr 2.130570e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 358 iter 22: train loss 0.70683. lr 2.136538e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 359 iter 22: train loss 0.71162. lr 2.142506e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 360 iter 22: train loss 0.69238. lr 2.148474e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 361 iter 22: train loss 0.69458. lr 2.154442e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 362 iter 22: train loss 0.71990. lr 2.160410e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 363 iter 22: train loss 0.69159. lr 2.166378e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 364 iter 22: train loss 0.70827. lr 2.172346e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 365 iter 22: train loss 0.69783. lr 2.178314e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 366 iter 22: train loss 0.70472. lr 2.184282e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 367 iter 22: train loss 0.71875. lr 2.190250e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 368 iter 22: train loss 0.73613. lr 2.196218e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 369 iter 22: train loss 0.68852. lr 2.202186e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 370 iter 22: train loss 0.67219. lr 2.208154e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 371 iter 22: train loss 0.72261. lr 2.214122e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 372 iter 22: train loss 0.69538. lr 2.220090e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 373 iter 22: train loss 0.69171. lr 2.226058e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 374 iter 22: train loss 0.69052. lr 2.232026e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 375 iter 22: train loss 0.69781. lr 2.237994e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 376 iter 22: train loss 0.71158. lr 2.243962e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 377 iter 22: train loss 0.72342. lr 2.249930e-03: 100% 23/23 [00:08<00:00,  2.86it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 378 iter 22: train loss 0.69614. lr 2.255898e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 379 iter 22: train loss 0.72518. lr 2.261866e-03: 100% 23/23 [00:08<00:00,  2.86it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 380 iter 22: train loss 0.70566. lr 2.267834e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 381 iter 22: train loss 0.67259. lr 2.273802e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 382 iter 22: train loss 0.67193. lr 2.279770e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 383 iter 22: train loss 0.68673. lr 2.285738e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 384 iter 22: train loss 0.66596. lr 2.291706e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 385 iter 22: train loss 0.69507. lr 2.297674e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 386 iter 22: train loss 0.66554. lr 2.303642e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 387 iter 22: train loss 0.70909. lr 2.309610e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 388 iter 22: train loss 0.65811. lr 2.315578e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 389 iter 22: train loss 0.66891. lr 2.321546e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 390 iter 22: train loss 0.69604. lr 2.327514e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 391 iter 22: train loss 0.68307. lr 2.333482e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 392 iter 22: train loss 0.68476. lr 2.339450e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 393 iter 22: train loss 0.71578. lr 2.345418e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 394 iter 22: train loss 0.68754. lr 2.351386e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 395 iter 22: train loss 0.69572. lr 2.357354e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 396 iter 22: train loss 0.65156. lr 2.363322e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 397 iter 22: train loss 0.70385. lr 2.369290e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 398 iter 22: train loss 0.67640. lr 2.375258e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 399 iter 22: train loss 0.70750. lr 2.381226e-03: 100% 23/23 [00:08<00:00,  2.86it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 400 iter 22: train loss 0.65560. lr 2.387194e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 401 iter 22: train loss 0.64564. lr 2.393162e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 402 iter 22: train loss 0.68731. lr 2.399130e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 403 iter 22: train loss 0.68473. lr 2.405098e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 404 iter 22: train loss 0.70621. lr 2.411066e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 405 iter 22: train loss 0.65833. lr 2.417034e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 406 iter 22: train loss 0.65667. lr 2.423002e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 407 iter 22: train loss 0.66574. lr 2.428969e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 408 iter 22: train loss 0.67803. lr 2.434937e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 409 iter 22: train loss 0.65437. lr 2.440905e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 410 iter 22: train loss 0.67391. lr 2.446873e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 411 iter 22: train loss 0.70901. lr 2.452841e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 412 iter 22: train loss 0.67693. lr 2.458809e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 413 iter 22: train loss 0.67746. lr 2.464777e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 414 iter 22: train loss 0.70480. lr 2.470745e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 415 iter 22: train loss 0.65508. lr 2.476713e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 416 iter 22: train loss 0.66016. lr 2.482681e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 417 iter 22: train loss 0.69335. lr 2.488649e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 418 iter 22: train loss 0.65984. lr 2.494617e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 419 iter 22: train loss 0.71002. lr 2.500585e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 420 iter 22: train loss 0.65340. lr 2.506553e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 421 iter 22: train loss 0.70456. lr 2.512521e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 422 iter 22: train loss 0.66933. lr 2.518489e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 423 iter 22: train loss 0.69240. lr 2.524457e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 424 iter 22: train loss 0.64731. lr 2.530425e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 425 iter 22: train loss 0.68276. lr 2.536393e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 426 iter 22: train loss 0.65610. lr 2.542361e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 427 iter 22: train loss 0.70827. lr 2.548329e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 428 iter 22: train loss 0.69251. lr 2.554297e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 429 iter 22: train loss 0.68916. lr 2.560265e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 430 iter 22: train loss 0.65040. lr 2.566233e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 431 iter 22: train loss 0.67381. lr 2.572201e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 432 iter 22: train loss 0.69212. lr 2.578169e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 433 iter 22: train loss 0.68170. lr 2.584137e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 434 iter 22: train loss 0.68007. lr 2.590105e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 435 iter 22: train loss 0.64863. lr 2.596073e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 436 iter 22: train loss 0.66381. lr 2.602041e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 437 iter 22: train loss 0.64364. lr 2.608009e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 438 iter 22: train loss 0.69174. lr 2.613977e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 439 iter 22: train loss 0.70017. lr 2.619945e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 440 iter 22: train loss 0.67510. lr 2.625913e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 441 iter 22: train loss 0.67368. lr 2.631881e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 442 iter 22: train loss 0.66736. lr 2.637849e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 443 iter 22: train loss 0.64581. lr 2.643817e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 444 iter 22: train loss 0.65354. lr 2.649785e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 445 iter 22: train loss 0.70168. lr 2.655753e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 446 iter 22: train loss 0.67776. lr 2.661721e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 447 iter 22: train loss 0.67127. lr 2.667689e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 448 iter 22: train loss 0.69178. lr 2.673657e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 449 iter 22: train loss 0.67645. lr 2.679625e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 450 iter 22: train loss 0.69645. lr 2.685593e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 451 iter 22: train loss 0.65256. lr 2.691561e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 452 iter 22: train loss 0.68425. lr 2.697529e-03: 100% 23/23 [00:08<00:00,  2.80it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 453 iter 22: train loss 0.64294. lr 2.703497e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 454 iter 22: train loss 0.65597. lr 2.709465e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 455 iter 22: train loss 0.65519. lr 2.715433e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 456 iter 22: train loss 0.64823. lr 2.721401e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 457 iter 22: train loss 0.66514. lr 2.727369e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 458 iter 22: train loss 0.66727. lr 2.733337e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 459 iter 22: train loss 0.65057. lr 2.739305e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 460 iter 22: train loss 0.64705. lr 2.745273e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 461 iter 22: train loss 0.65061. lr 2.751241e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 462 iter 22: train loss 0.67405. lr 2.757209e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 463 iter 22: train loss 0.65499. lr 2.763177e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 464 iter 22: train loss 0.66608. lr 2.769145e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 465 iter 22: train loss 0.66806. lr 2.775113e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 466 iter 22: train loss 0.68034. lr 2.781081e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 467 iter 22: train loss 0.68872. lr 2.787049e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 468 iter 22: train loss 0.68130. lr 2.793017e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 469 iter 22: train loss 0.63093. lr 2.798984e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 470 iter 22: train loss 0.71152. lr 2.804952e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 471 iter 22: train loss 0.65924. lr 2.810920e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 472 iter 22: train loss 0.68233. lr 2.816888e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 473 iter 22: train loss 0.65245. lr 2.822856e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 474 iter 22: train loss 0.65370. lr 2.828824e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 475 iter 22: train loss 0.66051. lr 2.834792e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 476 iter 22: train loss 0.67989. lr 2.840760e-03: 100% 23/23 [00:08<00:00,  2.80it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 477 iter 22: train loss 0.68303. lr 2.846728e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 478 iter 22: train loss 0.66437. lr 2.852696e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 479 iter 22: train loss 0.68322. lr 2.858664e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 480 iter 22: train loss 0.63161. lr 2.864632e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 481 iter 22: train loss 0.68573. lr 2.870600e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 482 iter 22: train loss 0.64740. lr 2.876568e-03: 100% 23/23 [00:08<00:00,  2.80it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 483 iter 22: train loss 0.64176. lr 2.882536e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 484 iter 22: train loss 0.68397. lr 2.888504e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 485 iter 22: train loss 0.68046. lr 2.894472e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 486 iter 22: train loss 0.66332. lr 2.900440e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 487 iter 22: train loss 0.71062. lr 2.906408e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 488 iter 22: train loss 0.69264. lr 2.912376e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 489 iter 22: train loss 0.66525. lr 2.918344e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 490 iter 22: train loss 0.63596. lr 2.924312e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 491 iter 22: train loss 0.64813. lr 2.930280e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 492 iter 22: train loss 0.68359. lr 2.936248e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 493 iter 22: train loss 0.64467. lr 2.942216e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 494 iter 22: train loss 0.65135. lr 2.948184e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 495 iter 22: train loss 0.66969. lr 2.954152e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 496 iter 22: train loss 0.69212. lr 2.960120e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 497 iter 22: train loss 0.67853. lr 2.966088e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 498 iter 22: train loss 0.64708. lr 2.972056e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 499 iter 22: train loss 0.63184. lr 2.978024e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 500 iter 22: train loss 0.66675. lr 2.983992e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 501 iter 22: train loss 0.66754. lr 2.989960e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 502 iter 22: train loss 0.67169. lr 2.995928e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 503 iter 22: train loss 0.67024. lr 3.001896e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 504 iter 22: train loss 0.64348. lr 3.007864e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 505 iter 22: train loss 0.65075. lr 3.013832e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 506 iter 22: train loss 0.66765. lr 3.019800e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 507 iter 22: train loss 0.68401. lr 3.025768e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 508 iter 22: train loss 0.62174. lr 3.031736e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 509 iter 22: train loss 0.66821. lr 3.037704e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 510 iter 22: train loss 0.66017. lr 3.043672e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 511 iter 22: train loss 0.65780. lr 3.049640e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 512 iter 22: train loss 0.63021. lr 3.055608e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 513 iter 22: train loss 0.63965. lr 3.061576e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 514 iter 22: train loss 0.67791. lr 3.067544e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 515 iter 22: train loss 0.65615. lr 3.073512e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 516 iter 22: train loss 0.68863. lr 3.079480e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 517 iter 22: train loss 0.66693. lr 3.085448e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 518 iter 22: train loss 0.66841. lr 3.091416e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 519 iter 22: train loss 0.68176. lr 3.097384e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 520 iter 22: train loss 0.63601. lr 3.103352e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 521 iter 22: train loss 0.62811. lr 3.109320e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 522 iter 22: train loss 0.66959. lr 3.115288e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 523 iter 22: train loss 0.68308. lr 3.121256e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 524 iter 22: train loss 0.66463. lr 3.127224e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 525 iter 22: train loss 0.69075. lr 3.133192e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 526 iter 22: train loss 0.66226. lr 3.139160e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 527 iter 22: train loss 0.64509. lr 3.145128e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 528 iter 22: train loss 0.64030. lr 3.151096e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 529 iter 22: train loss 0.67763. lr 3.157064e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 530 iter 22: train loss 0.67742. lr 3.163032e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 531 iter 22: train loss 0.68378. lr 3.169000e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 532 iter 22: train loss 0.63082. lr 3.174967e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 533 iter 22: train loss 0.66662. lr 3.180935e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 534 iter 22: train loss 0.65456. lr 3.186903e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 535 iter 22: train loss 0.63824. lr 3.192871e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 536 iter 22: train loss 0.68980. lr 3.198839e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 537 iter 22: train loss 0.64966. lr 3.204807e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 538 iter 22: train loss 0.69388. lr 3.210775e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 539 iter 22: train loss 0.68414. lr 3.216743e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 540 iter 22: train loss 0.64720. lr 3.222711e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 541 iter 22: train loss 0.63320. lr 3.228679e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 542 iter 22: train loss 0.66298. lr 3.234647e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 543 iter 22: train loss 0.67893. lr 3.240615e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 544 iter 22: train loss 0.65535. lr 3.246583e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 545 iter 22: train loss 0.64815. lr 3.252551e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 546 iter 22: train loss 0.65106. lr 3.258519e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 547 iter 22: train loss 0.61863. lr 3.264487e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 548 iter 22: train loss 0.62517. lr 3.270455e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 549 iter 22: train loss 0.66341. lr 3.276423e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 550 iter 22: train loss 0.62534. lr 3.282391e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 551 iter 22: train loss 0.64375. lr 3.288359e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 552 iter 22: train loss 0.65590. lr 3.294327e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 553 iter 22: train loss 0.65569. lr 3.300295e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 554 iter 22: train loss 0.62938. lr 3.306263e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 555 iter 22: train loss 0.65363. lr 3.312231e-03: 100% 23/23 [00:08<00:00,  2.80it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 556 iter 22: train loss 0.63078. lr 3.318199e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 557 iter 22: train loss 0.61186. lr 3.324167e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 558 iter 22: train loss 0.63031. lr 3.330135e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 559 iter 22: train loss 0.65092. lr 3.336103e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 560 iter 22: train loss 0.63403. lr 3.342071e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 561 iter 22: train loss 0.62922. lr 3.348039e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 562 iter 22: train loss 0.63999. lr 3.354007e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 563 iter 22: train loss 0.66076. lr 3.359975e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 564 iter 22: train loss 0.67053. lr 3.365943e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 565 iter 22: train loss 0.66670. lr 3.371911e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 566 iter 22: train loss 0.65717. lr 3.377879e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 567 iter 22: train loss 0.65871. lr 3.383847e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 568 iter 22: train loss 0.64997. lr 3.389815e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 569 iter 22: train loss 0.64543. lr 3.395783e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 570 iter 22: train loss 0.66147. lr 3.401751e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 571 iter 22: train loss 0.65967. lr 3.407719e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 572 iter 22: train loss 0.61783. lr 3.413687e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 573 iter 22: train loss 0.63602. lr 3.419655e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 574 iter 22: train loss 0.65336. lr 3.425623e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 575 iter 22: train loss 0.62790. lr 3.431591e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 576 iter 22: train loss 0.62178. lr 3.437559e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 577 iter 22: train loss 0.67464. lr 3.443527e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 578 iter 22: train loss 0.68535. lr 3.449495e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 579 iter 22: train loss 0.64830. lr 3.455463e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 580 iter 22: train loss 0.63503. lr 3.461431e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 581 iter 22: train loss 0.63912. lr 3.467399e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 582 iter 22: train loss 0.65749. lr 3.473367e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 583 iter 22: train loss 0.61523. lr 3.479335e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 584 iter 22: train loss 0.63632. lr 3.485303e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 585 iter 22: train loss 0.64221. lr 3.491271e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 586 iter 22: train loss 0.63609. lr 3.497239e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 587 iter 22: train loss 0.65902. lr 3.503207e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 588 iter 22: train loss 0.67105. lr 3.509175e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 589 iter 22: train loss 0.63454. lr 3.515143e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 590 iter 22: train loss 0.62599. lr 3.521111e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 591 iter 22: train loss 0.68193. lr 3.527079e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 592 iter 22: train loss 0.62475. lr 3.533047e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 593 iter 22: train loss 0.63154. lr 3.539015e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 594 iter 22: train loss 0.64625. lr 3.544982e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 595 iter 22: train loss 0.62129. lr 3.550950e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 596 iter 22: train loss 0.65525. lr 3.556918e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 597 iter 22: train loss 0.66433. lr 3.562886e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 598 iter 22: train loss 0.63444. lr 3.568854e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 599 iter 22: train loss 0.63256. lr 3.574822e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 600 iter 22: train loss 0.69165. lr 3.580790e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 601 iter 22: train loss 0.67037. lr 3.586758e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 602 iter 22: train loss 0.63930. lr 3.592726e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 603 iter 22: train loss 0.60688. lr 3.598694e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 604 iter 22: train loss 0.66682. lr 3.604662e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 605 iter 22: train loss 0.64187. lr 3.610630e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 606 iter 22: train loss 0.63971. lr 3.616598e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 607 iter 22: train loss 0.64857. lr 3.622566e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 608 iter 22: train loss 0.63162. lr 3.628534e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 609 iter 22: train loss 0.64740. lr 3.634502e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 610 iter 22: train loss 0.66278. lr 3.640470e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 611 iter 22: train loss 0.66007. lr 3.646438e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 612 iter 22: train loss 0.62427. lr 3.652406e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 613 iter 22: train loss 0.65839. lr 3.658374e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 614 iter 22: train loss 0.61931. lr 3.664342e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 615 iter 22: train loss 0.66643. lr 3.670310e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 616 iter 22: train loss 0.62291. lr 3.676278e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 617 iter 22: train loss 0.65378. lr 3.682246e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 618 iter 22: train loss 0.67645. lr 3.688214e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 619 iter 22: train loss 0.63913. lr 3.694182e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 620 iter 22: train loss 0.65057. lr 3.700150e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 621 iter 22: train loss 0.66091. lr 3.706118e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 622 iter 22: train loss 0.66895. lr 3.712086e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 623 iter 22: train loss 0.61159. lr 3.718054e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 624 iter 22: train loss 0.66251. lr 3.724022e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 625 iter 22: train loss 0.62728. lr 3.729990e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 626 iter 22: train loss 0.66738. lr 3.735958e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 627 iter 22: train loss 0.65336. lr 3.741926e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 628 iter 22: train loss 0.65820. lr 3.747894e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 629 iter 22: train loss 0.64527. lr 3.753862e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 630 iter 22: train loss 0.66860. lr 3.759830e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 631 iter 22: train loss 0.63227. lr 3.765798e-03: 100% 23/23 [00:08<00:00,  2.85it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 632 iter 22: train loss 0.66465. lr 3.771766e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 633 iter 22: train loss 0.65624. lr 3.777734e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 634 iter 22: train loss 0.66746. lr 3.783702e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 635 iter 22: train loss 0.66096. lr 3.789670e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 636 iter 22: train loss 0.64811. lr 3.795638e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 637 iter 22: train loss 0.64527. lr 3.801606e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 638 iter 22: train loss 0.67543. lr 3.807574e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 639 iter 22: train loss 0.64520. lr 3.813542e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 640 iter 22: train loss 0.62165. lr 3.819510e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 641 iter 22: train loss 0.66566. lr 3.825478e-03: 100% 23/23 [00:08<00:00,  2.86it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 642 iter 22: train loss 0.62952. lr 3.831446e-03: 100% 23/23 [00:08<00:00,  2.84it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 643 iter 22: train loss 0.63801. lr 3.837414e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 644 iter 22: train loss 0.67847. lr 3.843382e-03: 100% 23/23 [00:08<00:00,  2.81it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 645 iter 22: train loss 0.63955. lr 3.849350e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 646 iter 22: train loss 0.65271. lr 3.855318e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 647 iter 22: train loss 0.66207. lr 3.861286e-03: 100% 23/23 [00:08<00:00,  2.82it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 648 iter 22: train loss 0.65155. lr 3.867254e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 649 iter 22: train loss 0.63146. lr 3.873222e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n","<dataset.CharCorruptionDataset object at 0x7fb76cb99410>\n","epoch 650 iter 22: train loss 0.64180. lr 3.879190e-03: 100% 23/23 [00:08<00:00,  2.83it/s]\n"]}],"source":["!python /content/drive/MyDrive/speech_research/lecture/Assignment5_2021/student-new/src/run.py pretrain vanilla wiki.txt --writing_params_path vanilla.pretrain.params"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":897507,"status":"ok","timestamp":1635471690409,"user":{"displayName":"오승현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08073038130517070928"},"user_tz":-540},"id":"6NLtQV-Fp0Vq","outputId":"2e1e7420-5733-45cd-ca09-52f679a47a79"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello World\n","wiki.txt\n","vanilla.pretrain.params\n","vanilla.finetune.params\n","birth_places_train.tsv\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","epoch 1 iter 7: train loss 0.70995. lr 5.999844e-04: 100% 8/8 [00:05<00:00,  1.36it/s]\n","epoch 2 iter 7: train loss 0.49618. lr 5.999351e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 3 iter 7: train loss 0.39037. lr 5.998520e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 4 iter 7: train loss 0.32338. lr 5.997351e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 5 iter 7: train loss 0.28527. lr 5.995844e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 6 iter 7: train loss 0.19338. lr 5.993999e-04: 100% 8/8 [00:05<00:00,  1.41it/s]\n","epoch 7 iter 7: train loss 0.16298. lr 5.991818e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 8 iter 7: train loss 0.13772. lr 5.989299e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 9 iter 7: train loss 0.10611. lr 5.986444e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 10 iter 7: train loss 0.09670. lr 5.983252e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 11 iter 7: train loss 0.07614. lr 5.979723e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 12 iter 7: train loss 0.07121. lr 5.975860e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 13 iter 7: train loss 0.05670. lr 5.971660e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 14 iter 7: train loss 0.04040. lr 5.967127e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 15 iter 7: train loss 0.04139. lr 5.962258e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 16 iter 7: train loss 0.03944. lr 5.957056e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 17 iter 7: train loss 0.03311. lr 5.951521e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 18 iter 7: train loss 0.02682. lr 5.945654e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 19 iter 7: train loss 0.02681. lr 5.939454e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 20 iter 7: train loss 0.02792. lr 5.932923e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 21 iter 7: train loss 0.02005. lr 5.926062e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 22 iter 7: train loss 0.02547. lr 5.918871e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 23 iter 7: train loss 0.01997. lr 5.911352e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 24 iter 7: train loss 0.01824. lr 5.903504e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 25 iter 7: train loss 0.02125. lr 5.895329e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 26 iter 7: train loss 0.01825. lr 5.886828e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 27 iter 7: train loss 0.01859. lr 5.878002e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 28 iter 7: train loss 0.01467. lr 5.868851e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 29 iter 7: train loss 0.01820. lr 5.859378e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 30 iter 7: train loss 0.01781. lr 5.849582e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 31 iter 7: train loss 0.01013. lr 5.839465e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 32 iter 7: train loss 0.01104. lr 5.829028e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 33 iter 7: train loss 0.01646. lr 5.818272e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 34 iter 7: train loss 0.01570. lr 5.807199e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 35 iter 7: train loss 0.01325. lr 5.795810e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 36 iter 7: train loss 0.01574. lr 5.784106e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 37 iter 7: train loss 0.01086. lr 5.772087e-04: 100% 8/8 [00:05<00:00,  1.46it/s]\n","epoch 38 iter 7: train loss 0.01503. lr 5.759757e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 39 iter 7: train loss 0.00976. lr 5.747116e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 40 iter 7: train loss 0.01014. lr 5.734165e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 41 iter 7: train loss 0.01034. lr 5.720906e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 42 iter 7: train loss 0.00909. lr 5.707341e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 43 iter 7: train loss 0.01038. lr 5.693470e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 44 iter 7: train loss 0.00752. lr 5.679296e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 45 iter 7: train loss 0.01300. lr 5.664820e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 46 iter 7: train loss 0.00838. lr 5.650044e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 47 iter 7: train loss 0.01201. lr 5.634970e-04: 100% 8/8 [00:05<00:00,  1.46it/s]\n","epoch 48 iter 7: train loss 0.00834. lr 5.619598e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 49 iter 7: train loss 0.00544. lr 5.603932e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 50 iter 7: train loss 0.00639. lr 5.587972e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 51 iter 7: train loss 0.00972. lr 5.571720e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 52 iter 7: train loss 0.00760. lr 5.555179e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 53 iter 7: train loss 0.01088. lr 5.538350e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 54 iter 7: train loss 0.00814. lr 5.521235e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 55 iter 7: train loss 0.00598. lr 5.503835e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 56 iter 7: train loss 0.00835. lr 5.486154e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 57 iter 7: train loss 0.00612. lr 5.468193e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 58 iter 7: train loss 0.00912. lr 5.449953e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 59 iter 7: train loss 0.01154. lr 5.431438e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 60 iter 7: train loss 0.00597. lr 5.412648e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 61 iter 7: train loss 0.00512. lr 5.393587e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 62 iter 7: train loss 0.00623. lr 5.374256e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 63 iter 7: train loss 0.00423. lr 5.354657e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 64 iter 7: train loss 0.00410. lr 5.334794e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 65 iter 7: train loss 0.00498. lr 5.314667e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 66 iter 7: train loss 0.00863. lr 5.294279e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 67 iter 7: train loss 0.00538. lr 5.273633e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 68 iter 7: train loss 0.00397. lr 5.252731e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 69 iter 7: train loss 0.00610. lr 5.231575e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 70 iter 7: train loss 0.00498. lr 5.210167e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 71 iter 7: train loss 0.00474. lr 5.188511e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 72 iter 7: train loss 0.00581. lr 5.166608e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 73 iter 7: train loss 0.00255. lr 5.144461e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 74 iter 7: train loss 0.00505. lr 5.122072e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 75 iter 7: train loss 0.00515. lr 5.099444e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 1 iter 7: train loss 0.70836. lr 5.999844e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 2 iter 7: train loss 0.49741. lr 5.999351e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 3 iter 7: train loss 0.38972. lr 5.998520e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 4 iter 7: train loss 0.32063. lr 5.997351e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 5 iter 7: train loss 0.27297. lr 5.995844e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 6 iter 7: train loss 0.20013. lr 5.993999e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 7 iter 7: train loss 0.16863. lr 5.991818e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 8 iter 7: train loss 0.13385. lr 5.989299e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 9 iter 7: train loss 0.11873. lr 5.986444e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 10 iter 7: train loss 0.09460. lr 5.983252e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 11 iter 7: train loss 0.08561. lr 5.979723e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 12 iter 7: train loss 0.06415. lr 5.975860e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 13 iter 7: train loss 0.05702. lr 5.971660e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 14 iter 7: train loss 0.05132. lr 5.967127e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 15 iter 7: train loss 0.04384. lr 5.962258e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 16 iter 7: train loss 0.04041. lr 5.957056e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 17 iter 7: train loss 0.03816. lr 5.951521e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 18 iter 7: train loss 0.02674. lr 5.945654e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 19 iter 7: train loss 0.02718. lr 5.939454e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 20 iter 7: train loss 0.03711. lr 5.932923e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 21 iter 7: train loss 0.03382. lr 5.926062e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 22 iter 7: train loss 0.01661. lr 5.918871e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 23 iter 7: train loss 0.02522. lr 5.911352e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 24 iter 7: train loss 0.01953. lr 5.903504e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 25 iter 7: train loss 0.01998. lr 5.895329e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 26 iter 7: train loss 0.01239. lr 5.886828e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 27 iter 7: train loss 0.02290. lr 5.878002e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 28 iter 7: train loss 0.01312. lr 5.868851e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 29 iter 7: train loss 0.01062. lr 5.859378e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 30 iter 7: train loss 0.01581. lr 5.849582e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 31 iter 7: train loss 0.01562. lr 5.839465e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 32 iter 7: train loss 0.01575. lr 5.829028e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 33 iter 7: train loss 0.01514. lr 5.818272e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 34 iter 7: train loss 0.01313. lr 5.807199e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 35 iter 7: train loss 0.01520. lr 5.795810e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 36 iter 7: train loss 0.00963. lr 5.784106e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 37 iter 7: train loss 0.00919. lr 5.772087e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 38 iter 7: train loss 0.00802. lr 5.759757e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 39 iter 7: train loss 0.01291. lr 5.747116e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 40 iter 7: train loss 0.01396. lr 5.734165e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 41 iter 7: train loss 0.01035. lr 5.720906e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 42 iter 7: train loss 0.00893. lr 5.707341e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 43 iter 7: train loss 0.01065. lr 5.693470e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 44 iter 7: train loss 0.00845. lr 5.679296e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 45 iter 7: train loss 0.00913. lr 5.664820e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 46 iter 7: train loss 0.00894. lr 5.650044e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 47 iter 7: train loss 0.01274. lr 5.634970e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 48 iter 7: train loss 0.00441. lr 5.619598e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 49 iter 7: train loss 0.01247. lr 5.603932e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 50 iter 7: train loss 0.01302. lr 5.587972e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 51 iter 7: train loss 0.00668. lr 5.571720e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 52 iter 7: train loss 0.00831. lr 5.555179e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 53 iter 7: train loss 0.01046. lr 5.538350e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 54 iter 7: train loss 0.00908. lr 5.521235e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 55 iter 7: train loss 0.00588. lr 5.503835e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 56 iter 7: train loss 0.00807. lr 5.486154e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 57 iter 7: train loss 0.00592. lr 5.468193e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 58 iter 7: train loss 0.00905. lr 5.449953e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 59 iter 7: train loss 0.00528. lr 5.431438e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 60 iter 7: train loss 0.00611. lr 5.412648e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 61 iter 7: train loss 0.00779. lr 5.393587e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 62 iter 7: train loss 0.00543. lr 5.374256e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 63 iter 7: train loss 0.00829. lr 5.354657e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 64 iter 7: train loss 0.00688. lr 5.334794e-04: 100% 8/8 [00:05<00:00,  1.42it/s]\n","epoch 65 iter 7: train loss 0.00574. lr 5.314667e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 66 iter 7: train loss 0.00433. lr 5.294279e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 67 iter 7: train loss 0.00319. lr 5.273633e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 68 iter 7: train loss 0.00730. lr 5.252731e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 69 iter 7: train loss 0.01371. lr 5.231575e-04: 100% 8/8 [00:05<00:00,  1.43it/s]\n","epoch 70 iter 7: train loss 0.00591. lr 5.210167e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 71 iter 7: train loss 0.00328. lr 5.188511e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 72 iter 7: train loss 0.00789. lr 5.166608e-04: 100% 8/8 [00:05<00:00,  1.45it/s]\n","epoch 73 iter 7: train loss 0.00405. lr 5.144461e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 74 iter 7: train loss 0.00576. lr 5.122072e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n","epoch 75 iter 7: train loss 0.00755. lr 5.099444e-04: 100% 8/8 [00:05<00:00,  1.44it/s]\n"]}],"source":["!python /content/drive/MyDrive/speech_research/lecture/a5_2020/student-new/src/run.py finetune vanilla wiki.txt --reading_params_path vanilla.pretrain.params --writing_params_path vanilla.finetune.params --finetune_corpus_path birth_places_train.tsv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70572,"status":"ok","timestamp":1635472038273,"user":{"displayName":"오승현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08073038130517070928"},"user_tz":-540},"id":"lopycppXuUgc","outputId":"ff29a1c2-2eda-4a87-8040-522a88953707"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello World\n","wiki.txt\n","vanilla.finetune.params\n","birth_dev.tsv\n","vanilla.pretrain.dev.predictions\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","500it [01:06,  7.53it/s]\n","Correct: 113.0 out of 500.0: 22.6%\n"]}],"source":["!python /content/drive/MyDrive/speech_research/lecture/a5_2020/student-new/src/run.py evaluate vanilla wiki.txt --reading_params_path vanilla.finetune.params --eval_corpus_path birth_dev.tsv --outputs_path vanilla.pretrain.dev.predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60574,"status":"ok","timestamp":1635472504336,"user":{"displayName":"오승현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08073038130517070928"},"user_tz":-540},"id":"uaIL0lMWwKkV","outputId":"52e28dae-a28a-481e-e99f-0d35fbf25365"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello World\n","wiki.txt\n","vanilla.finetune.params\n","birth_test_inputs.tsv\n","vanilla.pretrain.test.predictions\n","data has 418351 characters, 256 unique.\n","number of parameters: 3323392\n","437it [00:56,  7.71it/s]\n","No gold birth places provided; returning (0,0)\n","Predictions written to /content/drive/MyDrive/speech_research/lecture/Assignment5_2021/student-new/vanilla.pretrain.test.predictions; no targets provided\n"]}],"source":["!python /content/drive/MyDrive/speech_research/lecture/a5_2020/student-new/src/run.py evaluate vanilla wiki.txt --reading_params_path vanilla.finetune.params --eval_corpus_path birth_test_inputs.tsv --outputs_path vanilla.pretrain.test.predictions"]},{"cell_type":"markdown","metadata":{"id":"7nR2MCi6wre0"},"source":["### Synthesize varient"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5023657,"status":"ok","timestamp":1636008452157,"user":{"displayName":"오승현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08073038130517070928"},"user_tz":-540},"id":"gkq0aetEwu8J","outputId":"518d6707-4265-4eeb-8d75-ec04b94b4a51"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello World\n","wiki.txt\n","synthesizer.pretrain.params\n","data has 418351 characters, 256 unique.\n","number of parameters: 3076988\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","epoch 1 iter 22: train loss 4.62595. lr 5.967984e-06: 100% 23/23 [00:07<00:00,  2.96it/s]\n","epoch 2 iter 22: train loss 4.28704. lr 1.193597e-05: 100% 23/23 [00:07<00:00,  3.08it/s]\n","epoch 3 iter 22: train loss 3.94487. lr 1.790395e-05: 100% 23/23 [00:07<00:00,  3.07it/s]\n","epoch 4 iter 22: train loss 3.71314. lr 2.387194e-05: 100% 23/23 [00:07<00:00,  3.07it/s]\n","epoch 5 iter 22: train loss 3.54493. lr 2.983992e-05: 100% 23/23 [00:07<00:00,  3.08it/s]\n","epoch 6 iter 22: train loss 3.39146. lr 3.580790e-05: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 7 iter 22: train loss 3.22800. lr 4.177589e-05: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 8 iter 22: train loss 3.08465. lr 4.774387e-05: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 9 iter 22: train loss 2.96088. lr 5.371186e-05: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 10 iter 22: train loss 2.88462. lr 5.967984e-05: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 11 iter 22: train loss 2.81514. lr 6.564782e-05: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 12 iter 22: train loss 2.72570. lr 7.161581e-05: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 13 iter 22: train loss 2.70028. lr 7.758379e-05: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 14 iter 22: train loss 2.63527. lr 8.355178e-05: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 15 iter 22: train loss 2.61112. lr 8.951976e-05: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 16 iter 22: train loss 2.57942. lr 9.548774e-05: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 17 iter 22: train loss 2.54941. lr 1.014557e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 18 iter 22: train loss 2.51863. lr 1.074237e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 19 iter 22: train loss 2.49046. lr 1.133917e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 20 iter 22: train loss 2.47502. lr 1.193597e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 21 iter 22: train loss 2.43313. lr 1.253277e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 22 iter 22: train loss 2.43953. lr 1.312956e-04: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 23 iter 22: train loss 2.38302. lr 1.372636e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 24 iter 22: train loss 2.34744. lr 1.432316e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 25 iter 22: train loss 2.36040. lr 1.491996e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 26 iter 22: train loss 2.31172. lr 1.551676e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 27 iter 22: train loss 2.32305. lr 1.611356e-04: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 28 iter 22: train loss 2.28171. lr 1.671036e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 29 iter 22: train loss 2.24074. lr 1.730715e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 30 iter 22: train loss 2.20984. lr 1.790395e-04: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 31 iter 22: train loss 2.17375. lr 1.850075e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 32 iter 22: train loss 2.17482. lr 1.909755e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 33 iter 22: train loss 2.11294. lr 1.969435e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 34 iter 22: train loss 2.11385. lr 2.029115e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 35 iter 22: train loss 2.08321. lr 2.088794e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 36 iter 22: train loss 2.08851. lr 2.148474e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 37 iter 22: train loss 2.04668. lr 2.208154e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 38 iter 22: train loss 2.00788. lr 2.267834e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 39 iter 22: train loss 1.98747. lr 2.327514e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 40 iter 22: train loss 1.99694. lr 2.387194e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 41 iter 22: train loss 1.97031. lr 2.446873e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 42 iter 22: train loss 1.94624. lr 2.506553e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 43 iter 22: train loss 1.95867. lr 2.566233e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 44 iter 22: train loss 1.95544. lr 2.625913e-04: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 45 iter 22: train loss 1.93282. lr 2.685593e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 46 iter 22: train loss 1.87599. lr 2.745273e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 47 iter 22: train loss 1.86610. lr 2.804952e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 48 iter 22: train loss 1.83967. lr 2.864632e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 49 iter 22: train loss 1.84825. lr 2.924312e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 50 iter 22: train loss 1.87230. lr 2.983992e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 51 iter 22: train loss 1.84644. lr 3.043672e-04: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 52 iter 22: train loss 1.80545. lr 3.103352e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 53 iter 22: train loss 1.81887. lr 3.163032e-04: 100% 23/23 [00:07<00:00,  3.07it/s]\n","epoch 54 iter 22: train loss 1.78307. lr 3.222711e-04: 100% 23/23 [00:07<00:00,  3.08it/s]\n","epoch 55 iter 22: train loss 1.78192. lr 3.282391e-04: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 56 iter 22: train loss 1.74624. lr 3.342071e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 57 iter 22: train loss 1.79307. lr 3.401751e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 58 iter 22: train loss 1.74695. lr 3.461431e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 59 iter 22: train loss 1.69562. lr 3.521111e-04: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 60 iter 22: train loss 1.74878. lr 3.580790e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 61 iter 22: train loss 1.70868. lr 3.640470e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 62 iter 22: train loss 1.70534. lr 3.700150e-04: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 63 iter 22: train loss 1.70673. lr 3.759830e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 64 iter 22: train loss 1.65517. lr 3.819510e-04: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 65 iter 22: train loss 1.67029. lr 3.879190e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 66 iter 22: train loss 1.63686. lr 3.938869e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 67 iter 22: train loss 1.63222. lr 3.998549e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 68 iter 22: train loss 1.57637. lr 4.058229e-04: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 69 iter 22: train loss 1.61001. lr 4.117909e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 70 iter 22: train loss 1.55478. lr 4.177589e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 71 iter 22: train loss 1.59122. lr 4.237269e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 72 iter 22: train loss 1.54726. lr 4.296948e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 73 iter 22: train loss 1.57709. lr 4.356628e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 74 iter 22: train loss 1.54169. lr 4.416308e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 75 iter 22: train loss 1.53065. lr 4.475988e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 76 iter 22: train loss 1.55282. lr 4.535668e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 77 iter 22: train loss 1.53859. lr 4.595348e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 78 iter 22: train loss 1.52038. lr 4.655028e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 79 iter 22: train loss 1.52637. lr 4.714707e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 80 iter 22: train loss 1.50612. lr 4.774387e-04: 100% 23/23 [00:07<00:00,  3.07it/s]\n","epoch 81 iter 22: train loss 1.52028. lr 4.834067e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 82 iter 22: train loss 1.45592. lr 4.893747e-04: 100% 23/23 [00:07<00:00,  3.07it/s]\n","epoch 83 iter 22: train loss 1.50478. lr 4.953427e-04: 100% 23/23 [00:07<00:00,  3.08it/s]\n","epoch 84 iter 22: train loss 1.47396. lr 5.013107e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 85 iter 22: train loss 1.45560. lr 5.072786e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 86 iter 22: train loss 1.46465. lr 5.132466e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 87 iter 22: train loss 1.45639. lr 5.192146e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 88 iter 22: train loss 1.43495. lr 5.251826e-04: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 89 iter 22: train loss 1.44993. lr 5.311506e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 90 iter 22: train loss 1.43043. lr 5.371186e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 91 iter 22: train loss 1.43876. lr 5.430865e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 92 iter 22: train loss 1.40246. lr 5.490545e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 93 iter 22: train loss 1.37793. lr 5.550225e-04: 100% 23/23 [00:07<00:00,  3.07it/s]\n","epoch 94 iter 22: train loss 1.33357. lr 5.609905e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 95 iter 22: train loss 1.37661. lr 5.669585e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 96 iter 22: train loss 1.35549. lr 5.729265e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 97 iter 22: train loss 1.37842. lr 5.788944e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 98 iter 22: train loss 1.38020. lr 5.848624e-04: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 99 iter 22: train loss 1.32599. lr 5.908304e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 100 iter 22: train loss 1.34530. lr 5.967984e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 101 iter 22: train loss 1.29814. lr 6.027664e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 102 iter 22: train loss 1.33564. lr 6.087344e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 103 iter 22: train loss 1.32993. lr 6.147024e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 104 iter 22: train loss 1.32021. lr 6.206703e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 105 iter 22: train loss 1.31376. lr 6.266383e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 106 iter 22: train loss 1.33174. lr 6.326063e-04: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 107 iter 22: train loss 1.30886. lr 6.385743e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 108 iter 22: train loss 1.26011. lr 6.445423e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 109 iter 22: train loss 1.30968. lr 6.505103e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 110 iter 22: train loss 1.25130. lr 6.564782e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 111 iter 22: train loss 1.26670. lr 6.624462e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 112 iter 22: train loss 1.22482. lr 6.684142e-04: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 113 iter 22: train loss 1.22855. lr 6.743822e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 114 iter 22: train loss 1.21262. lr 6.803502e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 115 iter 22: train loss 1.24021. lr 6.863182e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 116 iter 22: train loss 1.24037. lr 6.922861e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 117 iter 22: train loss 1.20393. lr 6.982541e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 118 iter 22: train loss 1.21337. lr 7.042221e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 119 iter 22: train loss 1.17250. lr 7.101901e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 120 iter 22: train loss 1.22152. lr 7.161581e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 121 iter 22: train loss 1.24581. lr 7.221261e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 122 iter 22: train loss 1.18518. lr 7.280940e-04: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 123 iter 22: train loss 1.20642. lr 7.340620e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 124 iter 22: train loss 1.18897. lr 7.400300e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 125 iter 22: train loss 1.21983. lr 7.459980e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 126 iter 22: train loss 1.18806. lr 7.519660e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 127 iter 22: train loss 1.15486. lr 7.579340e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 128 iter 22: train loss 1.17774. lr 7.639020e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 129 iter 22: train loss 1.15908. lr 7.698699e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 130 iter 22: train loss 1.16474. lr 7.758379e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 131 iter 22: train loss 1.14203. lr 7.818059e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 132 iter 22: train loss 1.15260. lr 7.877739e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 133 iter 22: train loss 1.11159. lr 7.937419e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 134 iter 22: train loss 1.16576. lr 7.997099e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 135 iter 22: train loss 1.13610. lr 8.056778e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 136 iter 22: train loss 1.13290. lr 8.116458e-04: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 137 iter 22: train loss 1.15307. lr 8.176138e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 138 iter 22: train loss 1.15652. lr 8.235818e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 139 iter 22: train loss 1.09556. lr 8.295498e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 140 iter 22: train loss 1.11046. lr 8.355178e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 141 iter 22: train loss 1.11285. lr 8.414857e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 142 iter 22: train loss 1.07146. lr 8.474537e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 143 iter 22: train loss 1.06667. lr 8.534217e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 144 iter 22: train loss 1.06573. lr 8.593897e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 145 iter 22: train loss 1.05916. lr 8.653577e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 146 iter 22: train loss 1.08421. lr 8.713257e-04: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 147 iter 22: train loss 1.08154. lr 8.772936e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 148 iter 22: train loss 1.10590. lr 8.832616e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 149 iter 22: train loss 1.07487. lr 8.892296e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 150 iter 22: train loss 1.09225. lr 8.951976e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 151 iter 22: train loss 1.09251. lr 9.011656e-04: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 152 iter 22: train loss 1.11197. lr 9.071336e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 153 iter 22: train loss 1.08738. lr 9.131016e-04: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 154 iter 22: train loss 1.07094. lr 9.190695e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 155 iter 22: train loss 1.08494. lr 9.250375e-04: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 156 iter 22: train loss 1.08073. lr 9.310055e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 157 iter 22: train loss 1.04751. lr 9.369735e-04: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 158 iter 22: train loss 1.04214. lr 9.429415e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 159 iter 22: train loss 1.07309. lr 9.489095e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 160 iter 22: train loss 1.10438. lr 9.548774e-04: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 161 iter 22: train loss 1.03081. lr 9.608454e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 162 iter 22: train loss 1.06305. lr 9.668134e-04: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 163 iter 22: train loss 1.09702. lr 9.727814e-04: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 164 iter 22: train loss 1.05882. lr 9.787494e-04: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 165 iter 22: train loss 1.07204. lr 9.847174e-04: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 166 iter 22: train loss 1.02848. lr 9.906853e-04: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 167 iter 22: train loss 1.02618. lr 9.966533e-04: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 168 iter 22: train loss 1.02503. lr 1.002621e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 169 iter 22: train loss 1.04138. lr 1.008589e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 170 iter 22: train loss 1.01343. lr 1.014557e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 171 iter 22: train loss 1.05128. lr 1.020525e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 172 iter 22: train loss 1.00629. lr 1.026493e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 173 iter 22: train loss 1.00136. lr 1.032461e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 174 iter 22: train loss 1.03417. lr 1.038429e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 175 iter 22: train loss 1.01414. lr 1.044397e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 176 iter 22: train loss 1.04132. lr 1.050365e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 177 iter 22: train loss 1.03349. lr 1.056333e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 178 iter 22: train loss 1.03036. lr 1.062301e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 179 iter 22: train loss 1.01307. lr 1.068269e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 180 iter 22: train loss 1.01093. lr 1.074237e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 181 iter 22: train loss 0.97945. lr 1.080205e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 182 iter 22: train loss 1.00490. lr 1.086173e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 183 iter 22: train loss 1.01689. lr 1.092141e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 184 iter 22: train loss 1.01311. lr 1.098109e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 185 iter 22: train loss 1.00693. lr 1.104077e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 186 iter 22: train loss 1.01569. lr 1.110045e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 187 iter 22: train loss 0.96078. lr 1.116013e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 188 iter 22: train loss 0.99041. lr 1.121981e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 189 iter 22: train loss 0.98887. lr 1.127949e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 190 iter 22: train loss 0.99217. lr 1.133917e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 191 iter 22: train loss 1.01287. lr 1.139885e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 192 iter 22: train loss 0.96192. lr 1.145853e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 193 iter 22: train loss 0.96254. lr 1.151821e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 194 iter 22: train loss 0.95962. lr 1.157789e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 195 iter 22: train loss 0.95619. lr 1.163757e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 196 iter 22: train loss 0.95858. lr 1.169725e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 197 iter 22: train loss 0.96452. lr 1.175693e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 198 iter 22: train loss 0.95671. lr 1.181661e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 199 iter 22: train loss 0.99674. lr 1.187629e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 200 iter 22: train loss 0.94377. lr 1.193597e-03: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 201 iter 22: train loss 0.94239. lr 1.199565e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 202 iter 22: train loss 0.94991. lr 1.205533e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 203 iter 22: train loss 0.95066. lr 1.211501e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 204 iter 22: train loss 0.93991. lr 1.217469e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 205 iter 22: train loss 0.94395. lr 1.223437e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 206 iter 22: train loss 0.93575. lr 1.229405e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 207 iter 22: train loss 0.94210. lr 1.235373e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 208 iter 22: train loss 0.95202. lr 1.241341e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 209 iter 22: train loss 0.97992. lr 1.247309e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 210 iter 22: train loss 0.96913. lr 1.253277e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 211 iter 22: train loss 0.94471. lr 1.259245e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 212 iter 22: train loss 0.95041. lr 1.265213e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 213 iter 22: train loss 0.93810. lr 1.271181e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 214 iter 22: train loss 0.94011. lr 1.277149e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 215 iter 22: train loss 0.92134. lr 1.283117e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 216 iter 22: train loss 0.96561. lr 1.289085e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 217 iter 22: train loss 0.89631. lr 1.295053e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 218 iter 22: train loss 0.87366. lr 1.301021e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 219 iter 22: train loss 0.90422. lr 1.306988e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 220 iter 22: train loss 0.91616. lr 1.312956e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 221 iter 22: train loss 0.91631. lr 1.318924e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 222 iter 22: train loss 0.93807. lr 1.324892e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 223 iter 22: train loss 0.94411. lr 1.330860e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 224 iter 22: train loss 0.91378. lr 1.336828e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 225 iter 22: train loss 0.90339. lr 1.342796e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 226 iter 22: train loss 0.91333. lr 1.348764e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 227 iter 22: train loss 0.91786. lr 1.354732e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 228 iter 22: train loss 0.92581. lr 1.360700e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 229 iter 22: train loss 0.92682. lr 1.366668e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 230 iter 22: train loss 0.90497. lr 1.372636e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 231 iter 22: train loss 0.94210. lr 1.378604e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 232 iter 22: train loss 0.92425. lr 1.384572e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 233 iter 22: train loss 0.89140. lr 1.390540e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 234 iter 22: train loss 0.89700. lr 1.396508e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 235 iter 22: train loss 0.93541. lr 1.402476e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 236 iter 22: train loss 0.87501. lr 1.408444e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 237 iter 22: train loss 0.91059. lr 1.414412e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 238 iter 22: train loss 0.84252. lr 1.420380e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 239 iter 22: train loss 0.86629. lr 1.426348e-03: 100% 23/23 [00:07<00:00,  2.96it/s]\n","epoch 240 iter 22: train loss 0.87940. lr 1.432316e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 241 iter 22: train loss 0.90199. lr 1.438284e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 242 iter 22: train loss 0.85687. lr 1.444252e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 243 iter 22: train loss 0.86056. lr 1.450220e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 244 iter 22: train loss 0.85675. lr 1.456188e-03: 100% 23/23 [00:07<00:00,  2.94it/s]\n","epoch 245 iter 22: train loss 0.86435. lr 1.462156e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 246 iter 22: train loss 0.90128. lr 1.468124e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 247 iter 22: train loss 0.88345. lr 1.474092e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 248 iter 22: train loss 0.86909. lr 1.480060e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 249 iter 22: train loss 0.88661. lr 1.486028e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 250 iter 22: train loss 0.88880. lr 1.491996e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 251 iter 22: train loss 0.91993. lr 1.497964e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 252 iter 22: train loss 0.85429. lr 1.503932e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 253 iter 22: train loss 0.86203. lr 1.509900e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 254 iter 22: train loss 0.86630. lr 1.515868e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 255 iter 22: train loss 0.85910. lr 1.521836e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 256 iter 22: train loss 0.85522. lr 1.527804e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 257 iter 22: train loss 0.85030. lr 1.533772e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 258 iter 22: train loss 0.87681. lr 1.539740e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 259 iter 22: train loss 0.90377. lr 1.545708e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 260 iter 22: train loss 0.85293. lr 1.551676e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 261 iter 22: train loss 0.86237. lr 1.557644e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 262 iter 22: train loss 0.86573. lr 1.563612e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 263 iter 22: train loss 0.83805. lr 1.569580e-03: 100% 23/23 [00:07<00:00,  2.96it/s]\n","epoch 264 iter 22: train loss 0.86705. lr 1.575548e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 265 iter 22: train loss 0.87794. lr 1.581516e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 266 iter 22: train loss 0.87194. lr 1.587484e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 267 iter 22: train loss 0.88856. lr 1.593452e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 268 iter 22: train loss 0.83121. lr 1.599420e-03: 100% 23/23 [00:07<00:00,  2.96it/s]\n","epoch 269 iter 22: train loss 0.85804. lr 1.605388e-03: 100% 23/23 [00:07<00:00,  2.96it/s]\n","epoch 270 iter 22: train loss 0.83811. lr 1.611356e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 271 iter 22: train loss 0.84768. lr 1.617324e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 272 iter 22: train loss 0.86215. lr 1.623292e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 273 iter 22: train loss 0.85203. lr 1.629260e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 274 iter 22: train loss 0.84330. lr 1.635228e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 275 iter 22: train loss 0.81934. lr 1.641196e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 276 iter 22: train loss 0.86175. lr 1.647164e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 277 iter 22: train loss 0.82774. lr 1.653132e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 278 iter 22: train loss 0.83349. lr 1.659100e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 279 iter 22: train loss 0.83085. lr 1.665068e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 280 iter 22: train loss 0.81129. lr 1.671036e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 281 iter 22: train loss 0.79042. lr 1.677004e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 282 iter 22: train loss 0.82767. lr 1.682971e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 283 iter 22: train loss 0.87654. lr 1.688939e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 284 iter 22: train loss 0.85075. lr 1.694907e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 285 iter 22: train loss 0.79120. lr 1.700875e-03: 100% 23/23 [00:07<00:00,  2.95it/s]\n","epoch 286 iter 22: train loss 0.82206. lr 1.706843e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 287 iter 22: train loss 0.84269. lr 1.712811e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 288 iter 22: train loss 0.82206. lr 1.718779e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 289 iter 22: train loss 0.82870. lr 1.724747e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 290 iter 22: train loss 0.83473. lr 1.730715e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 291 iter 22: train loss 0.84781. lr 1.736683e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 292 iter 22: train loss 0.83846. lr 1.742651e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 293 iter 22: train loss 0.80935. lr 1.748619e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 294 iter 22: train loss 0.78926. lr 1.754587e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 295 iter 22: train loss 0.83257. lr 1.760555e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 296 iter 22: train loss 0.84460. lr 1.766523e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 297 iter 22: train loss 0.82025. lr 1.772491e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 298 iter 22: train loss 0.81456. lr 1.778459e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 299 iter 22: train loss 0.80279. lr 1.784427e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 300 iter 22: train loss 0.84684. lr 1.790395e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 301 iter 22: train loss 0.80952. lr 1.796363e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 302 iter 22: train loss 0.81959. lr 1.802331e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 303 iter 22: train loss 0.80165. lr 1.808299e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 304 iter 22: train loss 0.81970. lr 1.814267e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 305 iter 22: train loss 0.79529. lr 1.820235e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 306 iter 22: train loss 0.80740. lr 1.826203e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 307 iter 22: train loss 0.80983. lr 1.832171e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 308 iter 22: train loss 0.79909. lr 1.838139e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 309 iter 22: train loss 0.83221. lr 1.844107e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 310 iter 22: train loss 0.84796. lr 1.850075e-03: 100% 23/23 [00:07<00:00,  2.94it/s]\n","epoch 311 iter 22: train loss 0.82168. lr 1.856043e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 312 iter 22: train loss 0.78113. lr 1.862011e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 313 iter 22: train loss 0.81477. lr 1.867979e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 314 iter 22: train loss 0.82778. lr 1.873947e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 315 iter 22: train loss 0.83466. lr 1.879915e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 316 iter 22: train loss 0.76381. lr 1.885883e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 317 iter 22: train loss 0.78943. lr 1.891851e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 318 iter 22: train loss 0.79707. lr 1.897819e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 319 iter 22: train loss 0.80072. lr 1.903787e-03: 100% 23/23 [00:07<00:00,  2.96it/s]\n","epoch 320 iter 22: train loss 0.80368. lr 1.909755e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 321 iter 22: train loss 0.78927. lr 1.915723e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 322 iter 22: train loss 0.76125. lr 1.921691e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 323 iter 22: train loss 0.78429. lr 1.927659e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 324 iter 22: train loss 0.78849. lr 1.933627e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 325 iter 22: train loss 0.82021. lr 1.939595e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 326 iter 22: train loss 0.78407. lr 1.945563e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 327 iter 22: train loss 0.79740. lr 1.951531e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 328 iter 22: train loss 0.80674. lr 1.957499e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 329 iter 22: train loss 0.79636. lr 1.963467e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 330 iter 22: train loss 0.76944. lr 1.969435e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 331 iter 22: train loss 0.78060. lr 1.975403e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 332 iter 22: train loss 0.76411. lr 1.981371e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 333 iter 22: train loss 0.82764. lr 1.987339e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 334 iter 22: train loss 0.77056. lr 1.993307e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 335 iter 22: train loss 0.80075. lr 1.999275e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 336 iter 22: train loss 0.78124. lr 2.005243e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 337 iter 22: train loss 0.82785. lr 2.011211e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 338 iter 22: train loss 0.77487. lr 2.017179e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 339 iter 22: train loss 0.78731. lr 2.023147e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 340 iter 22: train loss 0.76335. lr 2.029115e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 341 iter 22: train loss 0.78276. lr 2.035083e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 342 iter 22: train loss 0.80915. lr 2.041051e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 343 iter 22: train loss 0.74891. lr 2.047019e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 344 iter 22: train loss 0.78291. lr 2.052986e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 345 iter 22: train loss 0.75298. lr 2.058954e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 346 iter 22: train loss 0.77915. lr 2.064922e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 347 iter 22: train loss 0.77453. lr 2.070890e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 348 iter 22: train loss 0.76236. lr 2.076858e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 349 iter 22: train loss 0.80378. lr 2.082826e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 350 iter 22: train loss 0.80328. lr 2.088794e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 351 iter 22: train loss 0.75596. lr 2.094762e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 352 iter 22: train loss 0.74712. lr 2.100730e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 353 iter 22: train loss 0.78792. lr 2.106698e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 354 iter 22: train loss 0.74557. lr 2.112666e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 355 iter 22: train loss 0.81248. lr 2.118634e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 356 iter 22: train loss 0.77210. lr 2.124602e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 357 iter 22: train loss 0.77339. lr 2.130570e-03: 100% 23/23 [00:07<00:00,  2.97it/s]\n","epoch 358 iter 22: train loss 0.76944. lr 2.136538e-03: 100% 23/23 [00:07<00:00,  2.94it/s]\n","epoch 359 iter 22: train loss 0.75063. lr 2.142506e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 360 iter 22: train loss 0.77724. lr 2.148474e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 361 iter 22: train loss 0.72837. lr 2.154442e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 362 iter 22: train loss 0.79395. lr 2.160410e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 363 iter 22: train loss 0.77252. lr 2.166378e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 364 iter 22: train loss 0.77765. lr 2.172346e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 365 iter 22: train loss 0.80066. lr 2.178314e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 366 iter 22: train loss 0.78589. lr 2.184282e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 367 iter 22: train loss 0.76203. lr 2.190250e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 368 iter 22: train loss 0.77350. lr 2.196218e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 369 iter 22: train loss 0.78405. lr 2.202186e-03: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 370 iter 22: train loss 0.79296. lr 2.208154e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 371 iter 22: train loss 0.75150. lr 2.214122e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 372 iter 22: train loss 0.77482. lr 2.220090e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 373 iter 22: train loss 0.76310. lr 2.226058e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 374 iter 22: train loss 0.79528. lr 2.232026e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 375 iter 22: train loss 0.75320. lr 2.237994e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 376 iter 22: train loss 0.77124. lr 2.243962e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 377 iter 22: train loss 0.77114. lr 2.249930e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 378 iter 22: train loss 0.76170. lr 2.255898e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 379 iter 22: train loss 0.76316. lr 2.261866e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 380 iter 22: train loss 0.77449. lr 2.267834e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 381 iter 22: train loss 0.78084. lr 2.273802e-03: 100% 23/23 [00:07<00:00,  2.95it/s]\n","epoch 382 iter 22: train loss 0.74028. lr 2.279770e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 383 iter 22: train loss 0.73546. lr 2.285738e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 384 iter 22: train loss 0.78956. lr 2.291706e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 385 iter 22: train loss 0.76493. lr 2.297674e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 386 iter 22: train loss 0.77218. lr 2.303642e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 387 iter 22: train loss 0.76065. lr 2.309610e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 388 iter 22: train loss 0.80824. lr 2.315578e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 389 iter 22: train loss 0.76610. lr 2.321546e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 390 iter 22: train loss 0.75793. lr 2.327514e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 391 iter 22: train loss 0.75657. lr 2.333482e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 392 iter 22: train loss 0.75088. lr 2.339450e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 393 iter 22: train loss 0.75500. lr 2.345418e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 394 iter 22: train loss 0.77174. lr 2.351386e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 395 iter 22: train loss 0.79395. lr 2.357354e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 396 iter 22: train loss 0.76637. lr 2.363322e-03: 100% 23/23 [00:07<00:00,  2.96it/s]\n","epoch 397 iter 22: train loss 0.76972. lr 2.369290e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 398 iter 22: train loss 0.77018. lr 2.375258e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 399 iter 22: train loss 0.77753. lr 2.381226e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 400 iter 22: train loss 0.76507. lr 2.387194e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 401 iter 22: train loss 0.76751. lr 2.393162e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 402 iter 22: train loss 0.76924. lr 2.399130e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 403 iter 22: train loss 0.73948. lr 2.405098e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 404 iter 22: train loss 0.74022. lr 2.411066e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 405 iter 22: train loss 0.73383. lr 2.417034e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 406 iter 22: train loss 0.74372. lr 2.423002e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 407 iter 22: train loss 0.75913. lr 2.428969e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 408 iter 22: train loss 0.76243. lr 2.434937e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 409 iter 22: train loss 0.73589. lr 2.440905e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 410 iter 22: train loss 0.77986. lr 2.446873e-03: 100% 23/23 [00:07<00:00,  2.96it/s]\n","epoch 411 iter 22: train loss 0.76197. lr 2.452841e-03: 100% 23/23 [00:07<00:00,  2.96it/s]\n","epoch 412 iter 22: train loss 0.79683. lr 2.458809e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 413 iter 22: train loss 0.78253. lr 2.464777e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 414 iter 22: train loss 0.79828. lr 2.470745e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 415 iter 22: train loss 0.73481. lr 2.476713e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 416 iter 22: train loss 0.77362. lr 2.482681e-03: 100% 23/23 [00:07<00:00,  2.98it/s]\n","epoch 417 iter 22: train loss 0.76367. lr 2.488649e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 418 iter 22: train loss 0.73953. lr 2.494617e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 419 iter 22: train loss 0.74426. lr 2.500585e-03: 100% 23/23 [00:07<00:00,  2.99it/s]\n","epoch 420 iter 22: train loss 0.76064. lr 2.506553e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 421 iter 22: train loss 0.76586. lr 2.512521e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 422 iter 22: train loss 0.75598. lr 2.518489e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 423 iter 22: train loss 0.74081. lr 2.524457e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 424 iter 22: train loss 0.77544. lr 2.530425e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 425 iter 22: train loss 0.77980. lr 2.536393e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 426 iter 22: train loss 0.77506. lr 2.542361e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 427 iter 22: train loss 0.75369. lr 2.548329e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 428 iter 22: train loss 0.77503. lr 2.554297e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 429 iter 22: train loss 0.75548. lr 2.560265e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 430 iter 22: train loss 0.71649. lr 2.566233e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 431 iter 22: train loss 0.74417. lr 2.572201e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 432 iter 22: train loss 0.74842. lr 2.578169e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 433 iter 22: train loss 0.71846. lr 2.584137e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 434 iter 22: train loss 0.77262. lr 2.590105e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 435 iter 22: train loss 0.75143. lr 2.596073e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 436 iter 22: train loss 0.74386. lr 2.602041e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 437 iter 22: train loss 0.79941. lr 2.608009e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 438 iter 22: train loss 0.78225. lr 2.613977e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 439 iter 22: train loss 0.75433. lr 2.619945e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 440 iter 22: train loss 0.71877. lr 2.625913e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 441 iter 22: train loss 0.78013. lr 2.631881e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 442 iter 22: train loss 0.76686. lr 2.637849e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 443 iter 22: train loss 0.76378. lr 2.643817e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 444 iter 22: train loss 0.74955. lr 2.649785e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 445 iter 22: train loss 0.72580. lr 2.655753e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 446 iter 22: train loss 0.76534. lr 2.661721e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 447 iter 22: train loss 0.75724. lr 2.667689e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 448 iter 22: train loss 0.73992. lr 2.673657e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 449 iter 22: train loss 0.76589. lr 2.679625e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 450 iter 22: train loss 0.75798. lr 2.685593e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 451 iter 22: train loss 0.71838. lr 2.691561e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 452 iter 22: train loss 0.76307. lr 2.697529e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 453 iter 22: train loss 0.75210. lr 2.703497e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 454 iter 22: train loss 0.73667. lr 2.709465e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 455 iter 22: train loss 0.73272. lr 2.715433e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 456 iter 22: train loss 0.74820. lr 2.721401e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 457 iter 22: train loss 0.72601. lr 2.727369e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 458 iter 22: train loss 0.77053. lr 2.733337e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 459 iter 22: train loss 0.73097. lr 2.739305e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 460 iter 22: train loss 0.74692. lr 2.745273e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 461 iter 22: train loss 0.72265. lr 2.751241e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 462 iter 22: train loss 0.71724. lr 2.757209e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 463 iter 22: train loss 0.74145. lr 2.763177e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 464 iter 22: train loss 0.76605. lr 2.769145e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 465 iter 22: train loss 0.74779. lr 2.775113e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 466 iter 22: train loss 0.76080. lr 2.781081e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 467 iter 22: train loss 0.74026. lr 2.787049e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 468 iter 22: train loss 0.74413. lr 2.793017e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 469 iter 22: train loss 0.75098. lr 2.798984e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 470 iter 22: train loss 0.71702. lr 2.804952e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 471 iter 22: train loss 0.73936. lr 2.810920e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 472 iter 22: train loss 0.70882. lr 2.816888e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 473 iter 22: train loss 0.74374. lr 2.822856e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 474 iter 22: train loss 0.78179. lr 2.828824e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 475 iter 22: train loss 0.77258. lr 2.834792e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 476 iter 22: train loss 0.75417. lr 2.840760e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 477 iter 22: train loss 0.73052. lr 2.846728e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 478 iter 22: train loss 0.72280. lr 2.852696e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 479 iter 22: train loss 0.76237. lr 2.858664e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 480 iter 22: train loss 0.73636. lr 2.864632e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 481 iter 22: train loss 0.74915. lr 2.870600e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 482 iter 22: train loss 0.75011. lr 2.876568e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 483 iter 22: train loss 0.78454. lr 2.882536e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 484 iter 22: train loss 0.73753. lr 2.888504e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 485 iter 22: train loss 0.73240. lr 2.894472e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 486 iter 22: train loss 0.72186. lr 2.900440e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 487 iter 22: train loss 0.76619. lr 2.906408e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 488 iter 22: train loss 0.75442. lr 2.912376e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 489 iter 22: train loss 0.72238. lr 2.918344e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 490 iter 22: train loss 0.74763. lr 2.924312e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 491 iter 22: train loss 0.75106. lr 2.930280e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 492 iter 22: train loss 0.74776. lr 2.936248e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 493 iter 22: train loss 0.73759. lr 2.942216e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 494 iter 22: train loss 0.74074. lr 2.948184e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 495 iter 22: train loss 0.75436. lr 2.954152e-03: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 496 iter 22: train loss 0.71752. lr 2.960120e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 497 iter 22: train loss 0.74132. lr 2.966088e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 498 iter 22: train loss 0.77002. lr 2.972056e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 499 iter 22: train loss 0.74326. lr 2.978024e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 500 iter 22: train loss 0.73089. lr 2.983992e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 501 iter 22: train loss 0.75885. lr 2.989960e-03: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 502 iter 22: train loss 0.74701. lr 2.995928e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 503 iter 22: train loss 0.72276. lr 3.001896e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 504 iter 22: train loss 0.73844. lr 3.007864e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 505 iter 22: train loss 0.74173. lr 3.013832e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 506 iter 22: train loss 0.75461. lr 3.019800e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 507 iter 22: train loss 0.73928. lr 3.025768e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 508 iter 22: train loss 0.70842. lr 3.031736e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 509 iter 22: train loss 0.73665. lr 3.037704e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 510 iter 22: train loss 0.70989. lr 3.043672e-03: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 511 iter 22: train loss 0.74159. lr 3.049640e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 512 iter 22: train loss 0.77030. lr 3.055608e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 513 iter 22: train loss 0.73949. lr 3.061576e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 514 iter 22: train loss 0.73151. lr 3.067544e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 515 iter 22: train loss 0.71930. lr 3.073512e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 516 iter 22: train loss 0.70976. lr 3.079480e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 517 iter 22: train loss 0.70432. lr 3.085448e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 518 iter 22: train loss 0.72129. lr 3.091416e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 519 iter 22: train loss 0.73170. lr 3.097384e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 520 iter 22: train loss 0.71718. lr 3.103352e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 521 iter 22: train loss 0.73113. lr 3.109320e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 522 iter 22: train loss 0.76167. lr 3.115288e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 523 iter 22: train loss 0.75383. lr 3.121256e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 524 iter 22: train loss 0.68712. lr 3.127224e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 525 iter 22: train loss 0.73957. lr 3.133192e-03: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 526 iter 22: train loss 0.68918. lr 3.139160e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 527 iter 22: train loss 0.70501. lr 3.145128e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 528 iter 22: train loss 0.72178. lr 3.151096e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 529 iter 22: train loss 0.76404. lr 3.157064e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 530 iter 22: train loss 0.79371. lr 3.163032e-03: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 531 iter 22: train loss 0.71404. lr 3.169000e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 532 iter 22: train loss 0.76084. lr 3.174967e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 533 iter 22: train loss 0.76070. lr 3.180935e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 534 iter 22: train loss 0.73892. lr 3.186903e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 535 iter 22: train loss 0.72899. lr 3.192871e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 536 iter 22: train loss 0.70862. lr 3.198839e-03: 100% 23/23 [00:07<00:00,  3.07it/s]\n","epoch 537 iter 22: train loss 0.76765. lr 3.204807e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 538 iter 22: train loss 0.73181. lr 3.210775e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 539 iter 22: train loss 0.75289. lr 3.216743e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 540 iter 22: train loss 0.75484. lr 3.222711e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 541 iter 22: train loss 0.73914. lr 3.228679e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 542 iter 22: train loss 0.73211. lr 3.234647e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 543 iter 22: train loss 0.71389. lr 3.240615e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 544 iter 22: train loss 0.74089. lr 3.246583e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 545 iter 22: train loss 0.72421. lr 3.252551e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 546 iter 22: train loss 0.74426. lr 3.258519e-03: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 547 iter 22: train loss 0.73206. lr 3.264487e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 548 iter 22: train loss 0.75573. lr 3.270455e-03: 100% 23/23 [00:07<00:00,  3.07it/s]\n","epoch 549 iter 22: train loss 0.72437. lr 3.276423e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 550 iter 22: train loss 0.78717. lr 3.282391e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 551 iter 22: train loss 0.73467. lr 3.288359e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 552 iter 22: train loss 0.71830. lr 3.294327e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 553 iter 22: train loss 0.74572. lr 3.300295e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 554 iter 22: train loss 0.73347. lr 3.306263e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 555 iter 22: train loss 0.72042. lr 3.312231e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 556 iter 22: train loss 0.80068. lr 3.318199e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 557 iter 22: train loss 0.75322. lr 3.324167e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 558 iter 22: train loss 0.71072. lr 3.330135e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 559 iter 22: train loss 0.75404. lr 3.336103e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 560 iter 22: train loss 0.76483. lr 3.342071e-03: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 561 iter 22: train loss 0.71430. lr 3.348039e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 562 iter 22: train loss 0.75717. lr 3.354007e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 563 iter 22: train loss 0.72410. lr 3.359975e-03: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 564 iter 22: train loss 0.77641. lr 3.365943e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 565 iter 22: train loss 0.74401. lr 3.371911e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 566 iter 22: train loss 0.76439. lr 3.377879e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 567 iter 22: train loss 0.73503. lr 3.383847e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 568 iter 22: train loss 0.74039. lr 3.389815e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 569 iter 22: train loss 0.71215. lr 3.395783e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 570 iter 22: train loss 0.68620. lr 3.401751e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 571 iter 22: train loss 0.74819. lr 3.407719e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 572 iter 22: train loss 0.74162. lr 3.413687e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 573 iter 22: train loss 0.76709. lr 3.419655e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 574 iter 22: train loss 0.69998. lr 3.425623e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 575 iter 22: train loss 0.71581. lr 3.431591e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 576 iter 22: train loss 0.73570. lr 3.437559e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 577 iter 22: train loss 0.73262. lr 3.443527e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 578 iter 22: train loss 0.71465. lr 3.449495e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 579 iter 22: train loss 0.76233. lr 3.455463e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 580 iter 22: train loss 0.70944. lr 3.461431e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 581 iter 22: train loss 0.72491. lr 3.467399e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 582 iter 22: train loss 0.77552. lr 3.473367e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 583 iter 22: train loss 0.72216. lr 3.479335e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 584 iter 22: train loss 0.75779. lr 3.485303e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 585 iter 22: train loss 0.78712. lr 3.491271e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 586 iter 22: train loss 0.72796. lr 3.497239e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 587 iter 22: train loss 0.73803. lr 3.503207e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 588 iter 22: train loss 0.71977. lr 3.509175e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 589 iter 22: train loss 0.71255. lr 3.515143e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 590 iter 22: train loss 0.69672. lr 3.521111e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 591 iter 22: train loss 0.73692. lr 3.527079e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 592 iter 22: train loss 0.71897. lr 3.533047e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 593 iter 22: train loss 0.69655. lr 3.539015e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 594 iter 22: train loss 0.69797. lr 3.544982e-03: 100% 23/23 [00:07<00:00,  3.01it/s]\n","epoch 595 iter 22: train loss 0.74524. lr 3.550950e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 596 iter 22: train loss 0.76741. lr 3.556918e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 597 iter 22: train loss 0.71864. lr 3.562886e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 598 iter 22: train loss 0.73029. lr 3.568854e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 599 iter 22: train loss 0.71464. lr 3.574822e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 600 iter 22: train loss 0.76540. lr 3.580790e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 601 iter 22: train loss 0.73533. lr 3.586758e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 602 iter 22: train loss 0.76310. lr 3.592726e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 603 iter 22: train loss 0.73600. lr 3.598694e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 604 iter 22: train loss 0.73754. lr 3.604662e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 605 iter 22: train loss 0.74323. lr 3.610630e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 606 iter 22: train loss 0.70079. lr 3.616598e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 607 iter 22: train loss 0.69992. lr 3.622566e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 608 iter 22: train loss 0.71198. lr 3.628534e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 609 iter 22: train loss 0.72477. lr 3.634502e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 610 iter 22: train loss 0.72296. lr 3.640470e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 611 iter 22: train loss 0.69342. lr 3.646438e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 612 iter 22: train loss 0.77122. lr 3.652406e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 613 iter 22: train loss 0.72791. lr 3.658374e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 614 iter 22: train loss 0.72247. lr 3.664342e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 615 iter 22: train loss 0.74705. lr 3.670310e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 616 iter 22: train loss 0.73803. lr 3.676278e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 617 iter 22: train loss 0.71138. lr 3.682246e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 618 iter 22: train loss 0.73268. lr 3.688214e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 619 iter 22: train loss 0.72411. lr 3.694182e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 620 iter 22: train loss 0.71973. lr 3.700150e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 621 iter 22: train loss 0.72541. lr 3.706118e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 622 iter 22: train loss 0.72603. lr 3.712086e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 623 iter 22: train loss 0.71837. lr 3.718054e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 624 iter 22: train loss 0.75385. lr 3.724022e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 625 iter 22: train loss 0.73303. lr 3.729990e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 626 iter 22: train loss 0.71191. lr 3.735958e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 627 iter 22: train loss 0.72812. lr 3.741926e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 628 iter 22: train loss 0.74460. lr 3.747894e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 629 iter 22: train loss 0.76788. lr 3.753862e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 630 iter 22: train loss 0.74169. lr 3.759830e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 631 iter 22: train loss 0.70323. lr 3.765798e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 632 iter 22: train loss 0.72221. lr 3.771766e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 633 iter 22: train loss 0.70192. lr 3.777734e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 634 iter 22: train loss 0.71765. lr 3.783702e-03: 100% 23/23 [00:07<00:00,  3.06it/s]\n","epoch 635 iter 22: train loss 0.73475. lr 3.789670e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 636 iter 22: train loss 0.74438. lr 3.795638e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 637 iter 22: train loss 0.76553. lr 3.801606e-03: 100% 23/23 [00:07<00:00,  3.05it/s]\n","epoch 638 iter 22: train loss 0.76093. lr 3.807574e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 639 iter 22: train loss 0.73707. lr 3.813542e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 640 iter 22: train loss 0.74114. lr 3.819510e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 641 iter 22: train loss 0.70549. lr 3.825478e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 642 iter 22: train loss 0.70751. lr 3.831446e-03: 100% 23/23 [00:07<00:00,  3.02it/s]\n","epoch 643 iter 22: train loss 0.72966. lr 3.837414e-03: 100% 23/23 [00:07<00:00,  3.00it/s]\n","epoch 644 iter 22: train loss 0.72691. lr 3.843382e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 645 iter 22: train loss 0.75090. lr 3.849350e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 646 iter 22: train loss 0.74777. lr 3.855318e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 647 iter 22: train loss 0.73240. lr 3.861286e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n","epoch 648 iter 22: train loss 0.73859. lr 3.867254e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 649 iter 22: train loss 0.75962. lr 3.873222e-03: 100% 23/23 [00:07<00:00,  3.03it/s]\n","epoch 650 iter 22: train loss 0.75986. lr 3.879190e-03: 100% 23/23 [00:07<00:00,  3.04it/s]\n"]}],"source":["!python /content/drive/MyDrive/speech_research/lecture/a5_2020/student-new/src/run.py pretrain synthesizer wiki.txt --writing_params_path synthesizer.pretrain.params"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":773909,"status":"ok","timestamp":1636009305437,"user":{"displayName":"오승현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08073038130517070928"},"user_tz":-540},"id":"KhtIktofZDx3","outputId":"88b9350b-d00d-443e-c5c8-9523e94062a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello World\n","wiki.txt\n","synthesizer.pretrain.params\n","synthesizer.finetune.params\n","birth_places_train.tsv\n","data has 418351 characters, 256 unique.\n","number of parameters: 3076988\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","epoch 1 iter 7: train loss 0.81219. lr 5.999844e-04: 100% 8/8 [00:05<00:00,  1.55it/s]\n","epoch 2 iter 7: train loss 0.66254. lr 5.999351e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 3 iter 7: train loss 0.57410. lr 5.998520e-04: 100% 8/8 [00:04<00:00,  1.64it/s]\n","epoch 4 iter 7: train loss 0.50591. lr 5.997351e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 5 iter 7: train loss 0.45088. lr 5.995844e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 6 iter 7: train loss 0.40534. lr 5.993999e-04: 100% 8/8 [00:04<00:00,  1.62it/s]\n","epoch 7 iter 7: train loss 0.33177. lr 5.991818e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 8 iter 7: train loss 0.29693. lr 5.989299e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 9 iter 7: train loss 0.24645. lr 5.986444e-04: 100% 8/8 [00:04<00:00,  1.60it/s]\n","epoch 10 iter 7: train loss 0.19306. lr 5.983252e-04: 100% 8/8 [00:04<00:00,  1.60it/s]\n","epoch 11 iter 7: train loss 0.16315. lr 5.979723e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 12 iter 7: train loss 0.13899. lr 5.975860e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 13 iter 7: train loss 0.11042. lr 5.971660e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 14 iter 7: train loss 0.08017. lr 5.967127e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 15 iter 7: train loss 0.07257. lr 5.962258e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 16 iter 7: train loss 0.06811. lr 5.957056e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 17 iter 7: train loss 0.05396. lr 5.951521e-04: 100% 8/8 [00:04<00:00,  1.60it/s]\n","epoch 18 iter 7: train loss 0.05240. lr 5.945654e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 19 iter 7: train loss 0.04097. lr 5.939454e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 20 iter 7: train loss 0.03415. lr 5.932923e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 21 iter 7: train loss 0.02933. lr 5.926062e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 22 iter 7: train loss 0.03270. lr 5.918871e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 23 iter 7: train loss 0.03550. lr 5.911352e-04: 100% 8/8 [00:05<00:00,  1.57it/s]\n","epoch 24 iter 7: train loss 0.02794. lr 5.903504e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 25 iter 7: train loss 0.02205. lr 5.895329e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 26 iter 7: train loss 0.02570. lr 5.886828e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 27 iter 7: train loss 0.02525. lr 5.878002e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 28 iter 7: train loss 0.02086. lr 5.868851e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 29 iter 7: train loss 0.01403. lr 5.859378e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 30 iter 7: train loss 0.01726. lr 5.849582e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 31 iter 7: train loss 0.01710. lr 5.839465e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 32 iter 7: train loss 0.01660. lr 5.829028e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 33 iter 7: train loss 0.01080. lr 5.818272e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 34 iter 7: train loss 0.01533. lr 5.807199e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 35 iter 7: train loss 0.01420. lr 5.795810e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 36 iter 7: train loss 0.01380. lr 5.784106e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 37 iter 7: train loss 0.01293. lr 5.772087e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 38 iter 7: train loss 0.01319. lr 5.759757e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 39 iter 7: train loss 0.01007. lr 5.747116e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 40 iter 7: train loss 0.01195. lr 5.734165e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 41 iter 7: train loss 0.01017. lr 5.720906e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 42 iter 7: train loss 0.01422. lr 5.707341e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 43 iter 7: train loss 0.01372. lr 5.693470e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 44 iter 7: train loss 0.01169. lr 5.679296e-04: 100% 8/8 [00:05<00:00,  1.57it/s]\n","epoch 45 iter 7: train loss 0.01803. lr 5.664820e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 46 iter 7: train loss 0.01095. lr 5.650044e-04: 100% 8/8 [00:05<00:00,  1.57it/s]\n","epoch 47 iter 7: train loss 0.00574. lr 5.634970e-04: 100% 8/8 [00:04<00:00,  1.60it/s]\n","epoch 48 iter 7: train loss 0.00770. lr 5.619598e-04: 100% 8/8 [00:05<00:00,  1.57it/s]\n","epoch 49 iter 7: train loss 0.00600. lr 5.603932e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 50 iter 7: train loss 0.00757. lr 5.587972e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 51 iter 7: train loss 0.00793. lr 5.571720e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 52 iter 7: train loss 0.01262. lr 5.555179e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 53 iter 7: train loss 0.00874. lr 5.538350e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 54 iter 7: train loss 0.00534. lr 5.521235e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 55 iter 7: train loss 0.00844. lr 5.503835e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 56 iter 7: train loss 0.00684. lr 5.486154e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 57 iter 7: train loss 0.00484. lr 5.468193e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 58 iter 7: train loss 0.00920. lr 5.449953e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 59 iter 7: train loss 0.00733. lr 5.431438e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 60 iter 7: train loss 0.00903. lr 5.412648e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 61 iter 7: train loss 0.00828. lr 5.393587e-04: 100% 8/8 [00:04<00:00,  1.60it/s]\n","epoch 62 iter 7: train loss 0.00690. lr 5.374256e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 63 iter 7: train loss 0.00627. lr 5.354657e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 64 iter 7: train loss 0.00491. lr 5.334794e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 65 iter 7: train loss 0.00671. lr 5.314667e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 66 iter 7: train loss 0.00828. lr 5.294279e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 67 iter 7: train loss 0.00507. lr 5.273633e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 68 iter 7: train loss 0.00496. lr 5.252731e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 69 iter 7: train loss 0.00331. lr 5.231575e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 70 iter 7: train loss 0.00376. lr 5.210167e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 71 iter 7: train loss 0.00505. lr 5.188511e-04: 100% 8/8 [00:04<00:00,  1.62it/s]\n","epoch 72 iter 7: train loss 0.00351. lr 5.166608e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 73 iter 7: train loss 0.00642. lr 5.144461e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 74 iter 7: train loss 0.00376. lr 5.122072e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 75 iter 7: train loss 0.00282. lr 5.099444e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 1 iter 7: train loss 0.81754. lr 5.999844e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 2 iter 7: train loss 0.66161. lr 5.999351e-04: 100% 8/8 [00:04<00:00,  1.60it/s]\n","epoch 3 iter 7: train loss 0.58218. lr 5.998520e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 4 iter 7: train loss 0.51884. lr 5.997351e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 5 iter 7: train loss 0.44608. lr 5.995844e-04: 100% 8/8 [00:05<00:00,  1.57it/s]\n","epoch 6 iter 7: train loss 0.40017. lr 5.993999e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 7 iter 7: train loss 0.33570. lr 5.991818e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 8 iter 7: train loss 0.30323. lr 5.989299e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 9 iter 7: train loss 0.24272. lr 5.986444e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 10 iter 7: train loss 0.18414. lr 5.983252e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 11 iter 7: train loss 0.15248. lr 5.979723e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 12 iter 7: train loss 0.13332. lr 5.975860e-04: 100% 8/8 [00:04<00:00,  1.60it/s]\n","epoch 13 iter 7: train loss 0.11034. lr 5.971660e-04: 100% 8/8 [00:04<00:00,  1.60it/s]\n","epoch 14 iter 7: train loss 0.09886. lr 5.967127e-04: 100% 8/8 [00:04<00:00,  1.60it/s]\n","epoch 15 iter 7: train loss 0.07591. lr 5.962258e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 16 iter 7: train loss 0.06681. lr 5.957056e-04: 100% 8/8 [00:04<00:00,  1.60it/s]\n","epoch 17 iter 7: train loss 0.05098. lr 5.951521e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 18 iter 7: train loss 0.04606. lr 5.945654e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 19 iter 7: train loss 0.04552. lr 5.939454e-04: 100% 8/8 [00:05<00:00,  1.57it/s]\n","epoch 20 iter 7: train loss 0.04965. lr 5.932923e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 21 iter 7: train loss 0.03643. lr 5.926062e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 22 iter 7: train loss 0.02815. lr 5.918871e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 23 iter 7: train loss 0.02916. lr 5.911352e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 24 iter 7: train loss 0.03178. lr 5.903504e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 25 iter 7: train loss 0.01975. lr 5.895329e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 26 iter 7: train loss 0.02810. lr 5.886828e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 27 iter 7: train loss 0.02041. lr 5.878002e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 28 iter 7: train loss 0.01459. lr 5.868851e-04: 100% 8/8 [00:05<00:00,  1.57it/s]\n","epoch 29 iter 7: train loss 0.01319. lr 5.859378e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 30 iter 7: train loss 0.01734. lr 5.849582e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 31 iter 7: train loss 0.01174. lr 5.839465e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 32 iter 7: train loss 0.01338. lr 5.829028e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 33 iter 7: train loss 0.01197. lr 5.818272e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 34 iter 7: train loss 0.01667. lr 5.807199e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 35 iter 7: train loss 0.01340. lr 5.795810e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 36 iter 7: train loss 0.01446. lr 5.784106e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 37 iter 7: train loss 0.01983. lr 5.772087e-04: 100% 8/8 [00:05<00:00,  1.57it/s]\n","epoch 38 iter 7: train loss 0.01267. lr 5.759757e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 39 iter 7: train loss 0.00993. lr 5.747116e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 40 iter 7: train loss 0.01798. lr 5.734165e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 41 iter 7: train loss 0.01407. lr 5.720906e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 42 iter 7: train loss 0.01128. lr 5.707341e-04: 100% 8/8 [00:04<00:00,  1.60it/s]\n","epoch 43 iter 7: train loss 0.01048. lr 5.693470e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 44 iter 7: train loss 0.00896. lr 5.679296e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 45 iter 7: train loss 0.00809. lr 5.664820e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 46 iter 7: train loss 0.00966. lr 5.650044e-04: 100% 8/8 [00:05<00:00,  1.57it/s]\n","epoch 47 iter 7: train loss 0.00896. lr 5.634970e-04: 100% 8/8 [00:04<00:00,  1.60it/s]\n","epoch 48 iter 7: train loss 0.00944. lr 5.619598e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 49 iter 7: train loss 0.00737. lr 5.603932e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 50 iter 7: train loss 0.00914. lr 5.587972e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 51 iter 7: train loss 0.00705. lr 5.571720e-04: 100% 8/8 [00:04<00:00,  1.60it/s]\n","epoch 52 iter 7: train loss 0.00468. lr 5.555179e-04: 100% 8/8 [00:04<00:00,  1.61it/s]\n","epoch 53 iter 7: train loss 0.00870. lr 5.538350e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 54 iter 7: train loss 0.00411. lr 5.521235e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 55 iter 7: train loss 0.01129. lr 5.503835e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 56 iter 7: train loss 0.00546. lr 5.486154e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 57 iter 7: train loss 0.00526. lr 5.468193e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 58 iter 7: train loss 0.00563. lr 5.449953e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 59 iter 7: train loss 0.01041. lr 5.431438e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 60 iter 7: train loss 0.00541. lr 5.412648e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 61 iter 7: train loss 0.00394. lr 5.393587e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 62 iter 7: train loss 0.00698. lr 5.374256e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 63 iter 7: train loss 0.00661. lr 5.354657e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 64 iter 7: train loss 0.00883. lr 5.334794e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 65 iter 7: train loss 0.00521. lr 5.314667e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 66 iter 7: train loss 0.01249. lr 5.294279e-04: 100% 8/8 [00:05<00:00,  1.57it/s]\n","epoch 67 iter 7: train loss 0.00294. lr 5.273633e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 68 iter 7: train loss 0.00459. lr 5.252731e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 69 iter 7: train loss 0.00739. lr 5.231575e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 70 iter 7: train loss 0.00684. lr 5.210167e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n","epoch 71 iter 7: train loss 0.00471. lr 5.188511e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 72 iter 7: train loss 0.00523. lr 5.166608e-04: 100% 8/8 [00:05<00:00,  1.60it/s]\n","epoch 73 iter 7: train loss 0.00414. lr 5.144461e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 74 iter 7: train loss 0.00467. lr 5.122072e-04: 100% 8/8 [00:05<00:00,  1.59it/s]\n","epoch 75 iter 7: train loss 0.00319. lr 5.099444e-04: 100% 8/8 [00:05<00:00,  1.58it/s]\n"]}],"source":["!python /content/drive/MyDrive/speech_research/lecture/a5_2020/student-new/src/run.py finetune synthesizer wiki.txt --reading_params_path synthesizer.pretrain.params --writing_params_path synthesizer.finetune.params --finetune_corpus_path birth_places_train.tsv"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63496,"status":"ok","timestamp":1636009586970,"user":{"displayName":"오승현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08073038130517070928"},"user_tz":-540},"id":"XOYAwjkytTra","outputId":"6cfdc977-377b-4271-9754-bf87a86d6926"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello World\n","wiki.txt\n","synthesizer.finetune.params\n","birth_dev.tsv\n","synthesizer.pretrain.dev.predictions\n","data has 418351 characters, 256 unique.\n","number of parameters: 3076988\n","500it [00:59,  8.39it/s]\n","Correct: 56.0 out of 500.0: 11.200000000000001%\n"]}],"source":["!python /content/drive/MyDrive/speech_research/lecture/a5_2020/student-new/src/run.py evaluate synthesizer wiki.txt --reading_params_path synthesizer.finetune.params --eval_corpus_path birth_dev.tsv --outputs_path synthesizer.pretrain.dev.predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BMUbI6rrtjj0","outputId":"0570a983-f3b2-491f-8b9f-4944a6b35daf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hello World\n","wiki.txt\n","synthesizer.finetune.params\n","birth_test_inputs.tsv\n","synthesizer.pretrain.test.predictions\n","data has 418351 characters, 256 unique.\n","number of parameters: 3076988\n","387it [00:46,  8.67it/s]"]}],"source":["!python /content/drive/MyDrive/speech_research/lecture/a5_2020/student-new/src/run.py evaluate synthesizer wiki.txt --reading_params_path synthesizer.finetune.params --eval_corpus_path birth_test_inputs.tsv --outputs_path synthesizer.pretrain.test.predictions"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNfPmAlUGwFIy5BJ/Dt7Qd3","collapsed_sections":["2eqjXUx2z_38"],"name":"run.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
