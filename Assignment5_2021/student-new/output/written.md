# Written Part for Assignment 5
## 1. Attention exploration

- The reason, referred by [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)
    
    1. It expands the modelâ€™s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the the actual word itself. It would be useful if weâ€™re translating a sentence like â€œThe animal didnâ€™t cross the street because it was too tiredâ€, we would want to know which word â€œitâ€ refers to. 

    2. It gives the attention layer multiple â€œrepresentation subspacesâ€. As weâ€™ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.

**Multi-headed self-attention** is the core modeling component of Transformers. In this question, weâ€™ll get some practice working with the self-attention equations, and motivate **why multi-headed self-attention can be preferable to single-headed self-attention.**

(a) Copying in attention: Recall that attention can be viewed as an operation on a query $q âˆˆ \Reals^d$, a set of value vectors $\{v_1,...,v_n\},v_i âˆˆ \Reals^d$, and a set of key vectors $\{k_1,...,k_n\},k_i âˆˆ R_d$, specified as follows:

$c=\displaystyle\sum_{i=1}^{n}v_i\alpha_i$  (1)

$\alpha_i = \dfrac{exp(k_i^Tq)}{\sum_{j=1}^n exp(k_j^Tq)}$ (2)

whereÂ $Î±_i$Â are frequently called the â€œattention weightsâ€, and the outputÂ cÂ $âˆˆ \Reals^d$Â is a correspondingly weighted average over the value vectors. Weâ€™ll first show that itâ€™s particularly simple for attention to â€œcopyâ€ a value vector to the outputÂ c. 

Describe (in one sentence) **what properties of the inputs** to the attention operation would result in the outputÂ cÂ being approximately equal toÂ $v_j$Â for someÂ $j âˆˆ \{1, . . . , n\}$. Specifically, what must be true about the queryÂ q, the valuesÂ $\{v_1,...,v_n\}$Â and/or the keysÂ $\{k_1,...,k_n\}$?

- Answer
    
    $i\not=j, \ k_j^Tq \gg k_i^Tq$   Distribution of data is overweighted to a point.
    

(b) An average of two: Consider a set of key vectors $\{k_1, . . . , k_n\}$ where all key vectors are perpendicular, that is  $k_i âŠ¥ k_j$ for all $i \not= j$. Let $âˆ¥k_iâˆ¥ = 1$ for all i. Let $\{v_1,...,v_n\}$ be a set of arbitrary value vectors. Let $v_a, v_b âˆˆ \{v_1, . . . , v_n\}$ be two of the value vectors. Give an expression for a query vector q such that the output c is approximately equal to the average of $v_a$ and $v_b$, that is, $\frac{1}{2}(v_a +v_b)$.$^1$ Note that you can reference the corresponding key vector of $v_a$ and $v_b$ as $k_a$ and $k_b$.

- Answer
    
    $q=t(k_a+k_b),\ t \gg 0$
    

(c) Â Drawbacks of single-headed attention:Â In the previous part, **we saw how it wasÂ possibleÂ for single-headed attention to focus equally on two values**. The same concept could easily be extended to any subset of values. In this question weâ€™ll see why itâ€™s not aÂ practicalÂ solution. Consider a set of key vectorsÂ $\{k_1, . . . , k_n\}$Â that are now randomly sampled,Â $k_i âˆ¼ N(Î¼_i, Î£_i)$, where **the meansÂ $Î¼_i$Â are known to you, but the covariancesÂ $Î£_i$Â are unknown.** Further, assume that the meansÂ $Î¼_i$Â are all perpendicular;Â $Î¼^T_i Î¼_j$Â = 0Â 

ifÂ $i \not= j$, and unit norm,Â $âˆ¥Î¼_iâˆ¥ = 1$.

i.  Assume that the covariance matrices areÂ $Î£_i = Î±I$, for vanishingly smallÂ $Î±$. Design a queryÂ in terms of theÂ $Î¼_i$Â such that as before,Â $c â‰ˆ \frac{1}{2} (v_a + v_b)$, and provide a brief argument as to why it works.

- Answer
    
    $q=t(u_a+u_b),\ t\gg0$
    

ii.  Though single-headed attention is resistant to small perturbations in the keys, **some types of larger perturbations may pose a bigger issue.** Specifically, in some cases, one key vectorÂ $k_a$Â may be larger or smaller in norm than the others, while still pointing in the same direction asÂ $Ï€_a$Â . 

As an example, let us consider a covariance for itemÂ aÂ as $Î£_a = Î±I + \frac{1}{2}(Î¼_a Î¼^T_a)$Â for vanishingly smallÂ Î±Â (as shown in figureÂ 1). Further, letÂ $Î£iÂ =Â Î±I$Â for all  $i \not= a$.
When you sampleÂ $\{k_1 , . . . , k_n\}$Â multiple times, and use theÂ qÂ vector that you defined in part i., what qualitatively do you expect the vectorÂ cÂ will look like for different samples?

- Answer
    
    Shortly, if a side of input is biased, then it easily decided to that direction
    
    We got $k_{a} \sim \mathcal{N}\left(\mu_{a}, \alpha I+\frac{1}{2}\left(\mu_{a} \mu_{a}^{\top}\right)\right)$ ,and for vanishingly small $\alpha: k_{a} \approx \epsilon_{a} \mu_{a}, \epsilon_a \sim \mathcal{N}(1, \frac{1}{2}),\ when \ q = t(u_a+u_b), t\gg 0:$
    
    $k_i^Tq \approx 0 \text{ for } i \notin\{a, b\}$
    
    $k_a^Tq \approx \epsilon_a t$
    
    $k_b^Tq \approx \epsilon_b t$
    
    then:
    
    $\begin{aligned}c & \approx \frac{\exp (\epsilon_a t)}{\exp (\epsilon_a t)+\exp (\epsilon_b t)} v_{a}+\frac{\exp (\epsilon_b t)}{\exp (\epsilon_a t)+\exp (\epsilon_b t)} v_{b} \\&=\frac{1}{\exp ((\epsilon_b-\epsilon_a) t)+1} v_{a}+\frac{1}{\exp ((\epsilon_a-\epsilon_b) t)+1} v_{b}\end{aligned}$
    
    Since $\epsilon_a, \epsilon_b \sim \mathcal{N}(1, \frac{1}{2})$, when $\epsilon_a > \epsilon_b,\ c$ will be closer to $v_a$, vice versa.
    
    (i.e. $c$ will be closer to those with larger $||k||$)
    

(d) Â Benefits of multi-headed attention:Â Now weâ€™ll see some of the power of multi-headed attention. Weâ€™ll consider a simple version of multi-headed attention which is identical to single- headed self-attention as weâ€™ve presented it in this homework, except two query vectors ($q_1$Â andÂ $q_2$) are defined, which leads to a pair of vectors ($c_1$Â andÂ $c_2$), each the output of single-headed attention given its respective query vector. The final output of the multi-headed attention is their average,
$\frac{1}{2}(c_1+c_2)$. As in question 1(c), consider a set of key vectorsÂ $\{k_1 , . . . , k_n\}$ that are randomly sampled,

* Figure 1, Image is referred to assignment page

$k_i âˆ¼ N(Î¼_i,Î£_i)$, where the meansÂ $Î¼_i$Â are known to you, but the covariancesÂ $Î£_i$Â are unknown. Also as before, assume that the meansÂ $Î¼_i$Â are mutually orthogonal;Â $Î¼^T_i Î¼_j = 0$Â ifÂ $i\not= j$, and unit norm $âˆ¥Î¼_iâˆ¥ = 1$.

i. Assume that the covariance matrices areÂ $Î£_i = Î±I$, for vanishingly smallÂ $\alpha$. DesignÂ q1Â andÂ q2Â such thatÂ cÂ is approximately equal toÂ $\frac{1}{2} (v_a + v_b )$.

- Answer
    
    $q_a=t_1\mu_a,\ t_1 \gg 0$
    
    $q_b=t_2\mu_b,\ t_2\gg0$
    

ii. (2 points) Assume that the covariance matrices areÂ $Î£_a = Î±I + \frac{1}{2}(Î¼_aÎ¼^âŠ¤_a )$Â for vanishingly smallÂ $Î±$, andÂ $Î£_i = Î±I$  for allÂ $i\not= a$. Take the query vectorsÂ q1Â andÂ q2Â that you designed in part i. What, qualitatively, do you expect the outputÂ cÂ to look like across different samples of the key vectors? Please briefly explain why. You can ignore cases in whichÂ $q_i^Tk_a < 0$.

- Answer
    
    $k_a^T q = \varepsilon_at_1$
    
    $k_b^T q = \varepsilon_bt_2$
    
    Then,
    
    $c_1 \approx v_a, c_2 \approx v_b$
    
    $c = \frac{1}{2}(c_1+c_2) \approx \frac{1}{2}(v_a+v_b)$

    Because Multi-head states are a portion of filling with the state divided into multiple parts, it is possible to have the number of heads split.
    

(e)[TO-DO] Key-Query-Value self-attention in neural networks: So far, weâ€™ve discussed attention as a function on a set of key vectors, a set of value vectors, and a query vector. In Transformers, we perform self-attention, which roughly means that we draw the keys, values, and queries from the same data. More precisely, let $\{x_1,...,x_n\}$ be a sequence of vectors in $\Reals^d$. Think of each $x_i$ as representing word i in a sentence. One form of self-attention defines keys, queries, and values as follows. Let $V, K, Q âˆˆ \Reals^{dÃ—d}$ be parameter matrices. Then

$v_i= Vx_i,~i\in\{1,\dots,n\}$

$k_i= Kx_i,~i\in\{1,\dots,n\}$

$q_i= Qx_i,~i\in\{1,\dots,n\}$

Then we get a context vector for each input i; we have $c_i = âˆ‘^n_{j=1} Î±_{ij}v_j$, where $Î±_{ij}$ is defined as $Î± = \dfrac{exp(k_j^Tq_i)}{âˆ‘_{l=1}^nexp(k_l^Tq_i)}$  . Note that this is single-headed self-attention.

In this question, weâ€™ll show how key-value-query attention like this allows the network to use different aspects of the input vectorsÂ $x_i$Â in how it defines keys, queries, and values. **Intuitively, this allows networks to choose different aspects ofÂ $x_i$Â to be the â€œcontentâ€ (value vector) versus what it uses to determine â€œwhere to lookâ€œ for content (keys and queries.)**

i. First, consider if we didnâ€™t have key-query-value attention. For keys, queries, and values weâ€™ll just useÂ $x_i$; that is,Â $v_i = q_i = k_i = x_i$. Weâ€™ll consider a specific set ofÂ $x_i$. In particular, letÂ $u_a, u_b, u_c, u_d$Â be **mutually orthogonal vectors** inÂ $\Reals^d$, each with equal normÂ $âˆ¥u_aâˆ¥ = âˆ¥u_bâˆ¥ = âˆ¥u_câˆ¥ = âˆ¥u_dâˆ¥ = Î²$, whereÂ Î²Â is very large. Now, let ourÂ $x_i$Â be:

$x_1 = u_d+u_b$, (6)

$x_2=u_a$, (7)

$x_3=u_c+u_b$, (8)

If we perform self-attention with these vectors, what vector doesÂ $c_2$Â approximate? Would it be possible forÂ $c_2$Â to approximateÂ $u_b$Â by adding eitherÂ $u_d$Â orÂ $u_c$Â toÂ $x_2$? Explain why or why not (either math or English is fine).

- Answer
    
    $c_2 \approx u_a$
    
    It's impossible for $ğ‘_2$ to approximate $u_b$ by adding either $u_d$ or $u_c$ to $x_2$. 
    
    Say, if we add $u_d$ , $\alpha_{21}$ increases, which means the weight of $x_1$ increases, but $u_d$ and $u_b$ will increase equally in $c_2$, that's why $c_2$ can never be approximated to $u_b$
    

ii. Now consider using key-query-value attention as weâ€™ve defined it originally. **Using the same definitions ofÂ $x_1, x_2$Â andÂ $x_3$Â as in part i,** specify matricesÂ K, Q, VÂ such thatÂ $c_2 â‰ˆ u_b$, andÂ $c_1 â‰ˆ u_b âˆ’ u_c$. 

There are many solutions to this problem, so it will be easier for you (and the graders), if you first find VÂ such that $v_1 =u_b$Â and $v_3 =u_bâˆ’u_c$, then work on Q and K. Some outer product properties may be helpful (as summarized in this footnote)$^2$.

- Answer
    
    $V=u_b u_b^T \odot \dfrac{1}{\lVert
              u_b\rVert^2_2}-u_cu_c^T \odot \dfrac{1}{\lVert
              u_c\rVert^2_2}$
    
    $=u_b u_b^T \odot \dfrac{1}{\lVert
              u_b\rVert^2_2}-u_cu_c^T \odot \dfrac{1}{\lVert
              u_c\rVert^2_2}$
    
    $=(u_bu_b^T - u_cu_c^T)\odot \dfrac{1}{\beta^2}$
    
    $K=I$
    
    $Q=u_d u_a^T \odot \dfrac{1}{\lVert
              u_a\rVert^2_2}+u_cu_d^T \odot \dfrac{1}{\lVert
              u_d\rVert^2_2}$
    
    $=(u_du_a^T + u_cu_d^T)\odot \dfrac{1}{\beta^2}$
    
    Proof.
    
    $v_1=u_b, v_2=0, v_3 = u_b-u_c$
    
    $q_1=u_c,\ q_2=u_d,\ q_3=0$
    
    $k_i=x_i,\ i \in \{1,2,3\}$
    
    So,
    
    $\alpha_1 \approx [0,0,1],\ \alpha_2 \approx [1,0,1]$
    
    $c_1 \approx v_3 = u_b-u_c,\ c_2 \approx v_1 = u_b$